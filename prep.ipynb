{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab2b8df-2342-462a-9c7b-f39c2821a259",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Trip Duration\n",
    "\n",
    "Generate code & functions such that conducts data preprocessing(includes feature engineering & data cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defb9d45-f563-46f0-b387-cd90597809b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.56 s\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import Data Handling Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import Date-Time Handling Libraries\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "\n",
    "# Import Geodetic Libraries\n",
    "import pyproj\n",
    "from pyproj import Geod\n",
    "\n",
    "# Import Data Visualization Libraries\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.size\"] = 12\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 12]  # Set default figure size\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Machine Learning Libraries\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis\n",
    "\n",
    "# Set random seed for reproducibility in scikit-learn\n",
    "from sklearn.utils import check_random_state\n",
    "rng = check_random_state(42)\n",
    "\n",
    "# Import Utilities\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Import Custom Modules\n",
    "from data_loader import *  # Custom data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15788607-720c-45b5-854c-8e4f409eb38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: sys\n",
      "Loading train.csv from: E:\\Term 2\\[CEGE0042] Spatial-Temporal Data Analysis & Mining\\Assignment\\Repository\\data\\train.csv\n",
      "CPU times: total: 4.19 s\n",
      "Wall time: 4.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id0458976</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-29 18:21:02</td>\n",
       "      <td>2016-06-29 18:39:55</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.862762</td>\n",
       "      <td>40.768822</td>\n",
       "      <td>-73.891701</td>\n",
       "      <td>40.746689</td>\n",
       "      <td>N</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id0434613</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-25 13:03:26</td>\n",
       "      <td>2016-04-25 13:18:13</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.958038</td>\n",
       "      <td>40.783237</td>\n",
       "      <td>-73.975510</td>\n",
       "      <td>40.760853</td>\n",
       "      <td>N</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3809234</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-07 12:36:09</td>\n",
       "      <td>2016-05-07 12:47:35</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.969460</td>\n",
       "      <td>40.785519</td>\n",
       "      <td>-73.989243</td>\n",
       "      <td>40.771748</td>\n",
       "      <td>N</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id1203705</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-14 18:44:17</td>\n",
       "      <td>2016-05-14 18:57:55</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.981743</td>\n",
       "      <td>40.736549</td>\n",
       "      <td>-73.998352</td>\n",
       "      <td>40.726440</td>\n",
       "      <td>N</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1896645</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-10 22:51:25</td>\n",
       "      <td>2016-04-10 23:07:16</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.977913</td>\n",
       "      <td>40.752609</td>\n",
       "      <td>-73.975647</td>\n",
       "      <td>40.733139</td>\n",
       "      <td>N</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id0458976          2  2016-06-29 18:21:02  2016-06-29 18:39:55   \n",
       "1  id0434613          2  2016-04-25 13:03:26  2016-04-25 13:18:13   \n",
       "2  id3809234          2  2016-05-07 12:36:09  2016-05-07 12:47:35   \n",
       "3  id1203705          1  2016-05-14 18:44:17  2016-05-14 18:57:55   \n",
       "4  id1896645          2  2016-04-10 22:51:25  2016-04-10 23:07:16   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.862762        40.768822         -73.891701   \n",
       "1                1        -73.958038        40.783237         -73.975510   \n",
       "2                1        -73.969460        40.785519         -73.989243   \n",
       "3                1        -73.981743        40.736549         -73.998352   \n",
       "4                1        -73.977913        40.752609         -73.975647   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.746689                  N           1133  \n",
       "1         40.760853                  N            887  \n",
       "2         40.771748                  N            686  \n",
       "3         40.726440                  N            818  \n",
       "4         40.733139                  N            951  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the dataset\n",
    "df_train = load_data(\"train\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8107f50a-eb4c-4dd8-8488-71e99a0650ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: sys\n",
      "Loading test.csv from: E:\\Term 2\\[CEGE0042] Spatial-Temporal Data Analysis & Mining\\Assignment\\Repository\\data\\test.csv\n",
      "CPU times: total: 1.23 s\n",
      "Wall time: 1.23 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2793718</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-08 07:36:19</td>\n",
       "      <td>2016-06-08 07:53:39</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.985611</td>\n",
       "      <td>40.735943</td>\n",
       "      <td>-73.980331</td>\n",
       "      <td>40.760468</td>\n",
       "      <td>N</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3485529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-03 12:58:11</td>\n",
       "      <td>2016-04-03 13:11:58</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.978394</td>\n",
       "      <td>40.764351</td>\n",
       "      <td>-73.991623</td>\n",
       "      <td>40.749859</td>\n",
       "      <td>N</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1816614</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-05 02:49:13</td>\n",
       "      <td>2016-06-05 02:59:27</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.989059</td>\n",
       "      <td>40.744389</td>\n",
       "      <td>-73.973381</td>\n",
       "      <td>40.748692</td>\n",
       "      <td>N</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id1050851</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-05 17:18:27</td>\n",
       "      <td>2016-05-05 17:32:54</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.990326</td>\n",
       "      <td>40.731136</td>\n",
       "      <td>-73.991264</td>\n",
       "      <td>40.748917</td>\n",
       "      <td>N</td>\n",
       "      <td>867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id0140657</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-12 17:43:38</td>\n",
       "      <td>2016-05-12 19:06:25</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.789497</td>\n",
       "      <td>40.646675</td>\n",
       "      <td>-73.987137</td>\n",
       "      <td>40.759232</td>\n",
       "      <td>N</td>\n",
       "      <td>4967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2793718          2  2016-06-08 07:36:19  2016-06-08 07:53:39   \n",
       "1  id3485529          2  2016-04-03 12:58:11  2016-04-03 13:11:58   \n",
       "2  id1816614          2  2016-06-05 02:49:13  2016-06-05 02:59:27   \n",
       "3  id1050851          2  2016-05-05 17:18:27  2016-05-05 17:32:54   \n",
       "4  id0140657          1  2016-05-12 17:43:38  2016-05-12 19:06:25   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.985611        40.735943         -73.980331   \n",
       "1                1        -73.978394        40.764351         -73.991623   \n",
       "2                5        -73.989059        40.744389         -73.973381   \n",
       "3                2        -73.990326        40.731136         -73.991264   \n",
       "4                4        -73.789497        40.646675         -73.987137   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.760468                  N           1040  \n",
       "1         40.749859                  N            827  \n",
       "2         40.748692                  N            614  \n",
       "3         40.748917                  N            867  \n",
       "4         40.759232                  N           4967  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the test dataset\n",
    "df_test = load_data(\"test\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34d95c2-ffb2-4507-a23f-bd072c4ba113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 109 ms\n",
      "Wall time: 111 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Delete cols that leads to data leakage\n",
    "del df_train[\"dropoff_datetime\"]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b1bff-dd6d-4481-9a9b-473b2815cbb0",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71558b23-63ca-4c2e-88f7-7f91270f0ce5",
   "metadata": {},
   "source": [
    "### PCA in Longitudes & Latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75e8fa9-b231-47ee-b54b-7557f0d3b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.17 s\n",
      "Wall time: 533 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def apply_pca_to_coords(train, test, random_seed=42):\n",
    "    \"\"\"\n",
    "    Applies PCA transformation to pickup and dropoff coordinates for train and test datasets.\n",
    "\n",
    "    The PCA is fitted **only on the training data** to prevent data leakage.\n",
    "\n",
    "    Parameters:\n",
    "        train (pd.DataFrame): The training dataset.\n",
    "        test (pd.DataFrame): The testing dataset.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies train and test DataFrames in place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit PCA only on training data\n",
    "    coords_train = np.vstack((\n",
    "        train[[\"pickup_latitude\", \"pickup_longitude\"]].values,\n",
    "        train[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    ))\n",
    "\n",
    "    pca = PCA(whiten=True, random_state=random_seed).fit(coords_train)\n",
    "\n",
    "    # Apply transformation to train dataset\n",
    "    train_coords_pickup = train[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "    train_coords_dropoff = train[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    train.loc[:, \"pickup_pca0\"] = pca.transform(train_coords_pickup)[:, 0]\n",
    "    train.loc[:, \"pickup_pca1\"] = pca.transform(train_coords_pickup)[:, 1]\n",
    "    train.loc[:, \"dropoff_pca0\"] = pca.transform(train_coords_dropoff)[:, 0]\n",
    "    train.loc[:, \"dropoff_pca1\"] = pca.transform(train_coords_dropoff)[:, 1]\n",
    "\n",
    "    # Apply the same transformation to test dataset to avoid data leakage\n",
    "    test_coords_pickup = test[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "    test_coords_dropoff = test[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    test.loc[:, \"pickup_pca0\"] = pca.transform(test_coords_pickup)[:, 0]\n",
    "    test.loc[:, \"pickup_pca1\"] = pca.transform(test_coords_pickup)[:, 1]\n",
    "    test.loc[:, \"dropoff_pca0\"] = pca.transform(test_coords_dropoff)[:, 0]\n",
    "    test.loc[:, \"dropoff_pca1\"] = pca.transform(test_coords_dropoff)[:, 1]\n",
    "\n",
    "# Example usage:\n",
    "apply_pca_to_coords(df_train, df_test, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814f521-b108-4543-91a0-5095f831fb1c",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7a9992-3bcc-422a-9c7d-8318cbaa6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 38.9 s\n",
      "Wall time: 38.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define WGS84 ellipsoid\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Compute great-circle distance in kilometers\n",
    "df_train[\"euclidean_distance\"] = df_train.apply(\n",
    "    lambda row: geod.inv(row[\"pickup_longitude\"], row[\"pickup_latitude\"],\n",
    "                         row[\"dropoff_longitude\"], row[\"dropoff_latitude\"])[2] / 1000, axis=1\n",
    ")\n",
    "\n",
    "# Compute great-circle distance in kilometers\n",
    "df_test[\"euclidean_distance\"] = df_test.apply(\n",
    "    lambda row: geod.inv(row[\"pickup_longitude\"], row[\"pickup_latitude\"],\n",
    "                         row[\"dropoff_longitude\"], row[\"dropoff_latitude\"])[2] / 1000, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c2049-3816-4d41-9718-fdd59f412c65",
   "metadata": {},
   "source": [
    "### Datetime Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c665f53-2be9-4270-b4a3-bada27f573e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.83 s\n",
      "Wall time: 1.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generate_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Generate detailed date-time features for pickups and modify the DataFrame in place.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the datetime column.\n",
    "    \n",
    "    Returns:\n",
    "        None (Modifies df in place)\n",
    "    \"\"\"\n",
    "    # Convert to datetime format\n",
    "    pickup_times = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "\n",
    "    # Extract relevant time features\n",
    "    df[\"pickup_hour_of_day\"] = (pickup_times.dt.hour * 60.0 + pickup_times.dt.minute) / 60.0\n",
    "\n",
    "    df[\"day_of_week\"] = pickup_times.dt.weekday\n",
    "    df[\"hour_of_week\"] = df[\"day_of_week\"] * 24.0 + df[\"pickup_hour_of_day\"]\n",
    "\n",
    "    df[\"month_of_year\"] = pickup_times.dt.month\n",
    "    df[\"day_of_year\"] = pickup_times.dt.dayofyear\n",
    "    df[\"week_of_year\"] = pickup_times.dt.isocalendar().week\n",
    "    df[\"hour_of_year\"] = df[\"day_of_year\"] * 24.0 + df[\"pickup_hour_of_day\"]\n",
    "\n",
    "generate_datetime_features(df_train)\n",
    "generate_datetime_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729d5e0-34b6-41db-a562-fa3bfa155c86",
   "metadata": {},
   "source": [
    "### NYC Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23f5fb74-b94c-47a8-884e-3fdd1e13453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.3 s\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def merge_weather_data(df):\n",
    "    \"\"\"\n",
    "    Merges weather data with a given dataframe (train or test) based on the pickup date.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The train or test dataframe containing 'pickup_datetime'.\n",
    "    weather_data (pd.DataFrame): The weather data dataframe containing 'date'.\n",
    "\n",
    "    Returns:\n",
    "        None (Modifies df in place)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load nyc weather data to enrich information\n",
    "    weather_data = pd.read_csv(os.path.join(\"utils\",\n",
    "                                            \"weather_data_nyc_centralpark_2016.csv\"))\n",
    "    weather_data[\"date\"] = pd.to_datetime(weather_data[\"date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Ensure datetime consistency in datetime format\n",
    "    weather_data[\"date\"] = weather_data[\"date\"].dt.date\n",
    "    df[\"pickup_date\"] = pd.to_datetime(df[\"pickup_datetime\"]).dt.date\n",
    "\n",
    "    # Merge datasets on the date\n",
    "    df = df.merge(weather_data, left_on=\"pickup_date\", right_on=\"date\", how=\"left\")\n",
    "\n",
    "    # Drop redundant date columns\n",
    "    df.drop(columns=[\"pickup_date\"], inplace=True)\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "merge_weather_data(df_train)\n",
    "merge_weather_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10577d-68e2-44e6-9a4b-191c14a6e0c0",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c71eaa-60cb-4292-83b1-af4296aeb41f",
   "metadata": {},
   "source": [
    "### Location Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12284ae-e52d-453c-9129-0d4d3aa5c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before filtering: 1166915\n",
      "Records after filtering: 1166864\n",
      "Records dropped: 51\n",
      "\n",
      "Records before filtering: 291729\n",
      "Records after filtering: 291713\n",
      "Records dropped: 16\n",
      "\n",
      "CPU times: total: 703 ms\n",
      "Wall time: 701 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_nyc_boundary(df, geojson_path):\n",
    "    \"\"\"\n",
    "    Filters pickup and dropoff locations to keep only those within the New York City boundary.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing pickup and dropoff coordinates.\n",
    "        geojson_path (str): Path to the GeoJSON file defining NYC boundaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with locations inside the NYC bounding box.\n",
    "    \"\"\"\n",
    "    # Load the GeoJSON file\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    # Extract NYC boundary coordinates where NAME is \"New York\"\n",
    "    nyc_coords = []\n",
    "    for feature in geojson_data[\"features\"]:\n",
    "        if feature[\"properties\"].get(\"NAME\") == \"New York\":\n",
    "            for polygon in feature[\"geometry\"][\"coordinates\"]:  # Loop through MultiPolygon\n",
    "                for ring in polygon:  # Each polygon has a ring of coordinates\n",
    "                    nyc_coords.extend(ring)\n",
    "\n",
    "    # Compute NYC bounding box (min/max latitudes & longitudes)\n",
    "    min_long = min(lon for lon, lat in nyc_coords)\n",
    "    max_long = max(lon for lon, lat in nyc_coords)\n",
    "    min_lat = min(lat for lon, lat in nyc_coords)\n",
    "    max_lat = max(lat for lon, lat in nyc_coords)\n",
    "\n",
    "    # Count records before filtering\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Filter data based on bounding box\n",
    "    mask = (\n",
    "        (df[\"pickup_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"pickup_latitude\"].between(min_lat, max_lat))\n",
    "        & (df[\"dropoff_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"dropoff_latitude\"].between(min_lat, max_lat))\n",
    "    )\n",
    "\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    # Count records after filtering\n",
    "    final_count = len(filtered_df)\n",
    "    dropped_count = initial_count - final_count\n",
    "\n",
    "    print(f\"Records before filtering: {initial_count}\")\n",
    "    print(f\"Records after filtering: {final_count}\")\n",
    "    print(f\"Records dropped: {dropped_count}\\n\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Apply function to df_train and df_test\n",
    "df_train = filter_by_nyc_boundary(df_train, \"utils/gz_2010_us_040_00_5m.json\")\n",
    "df_test = filter_by_nyc_boundary(df_test, \"utils/gz_2010_us_040_00_5m.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d40a5-30bb-4057-bb02-02110a4556ac",
   "metadata": {},
   "source": [
    "### Duration Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c67dade-e69d-415d-8fa6-1927b0346552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before filtering: 1166864\n",
      "Records after filtering: 1164588\n",
      "Records dropped due to duration limits: 2276\n",
      "CPU times: total: 250 ms\n",
      "Wall time: 285 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_duration_range(df, lower_lim, upper_lim):\n",
    "    \"\"\"\n",
    "    Filters trips based on duration range.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing trip durations.\n",
    "        lower_lim (float): The minimum allowable duration.\n",
    "        upper_lim (float): The maximum allowable duration.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with durations within the specified range.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Apply duration filter\n",
    "    filtered_df = df[df[\"trip_duration\"].between(lower_lim, upper_lim)]\n",
    "\n",
    "    final_count = len(filtered_df)\n",
    "    dropped_count = initial_count - final_count\n",
    "\n",
    "    print(f\"Records before filtering: {initial_count}\")\n",
    "    print(f\"Records after filtering: {final_count}\")\n",
    "    print(f\"Records dropped due to duration limits: {dropped_count}\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Apply function to df_train\n",
    "df_train = filter_by_duration_range(df_train, 5, 36000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00651e3-34a4-4264-8012-d7b59958eda4",
   "metadata": {},
   "source": [
    "### Spatial & Temporal Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e1fa5-2b56-4787-bc95-6ce8b70a2115",
   "metadata": {},
   "source": [
    "- Spatial Aggregation of Trip Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0882b0-18ae-4385-83ff-d5099f2da1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 61.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def bin_coordinates(df, precision=2):\n",
    "    \"\"\"Bins latitude and longitude to a specified precision.\"\"\"\n",
    "    df.loc[:, \"pickup_lat_bin\"] = np.round(df[\"pickup_latitude\"], precision)\n",
    "    df.loc[:, \"pickup_long_bin\"] = np.round(df[\"pickup_longitude\"], precision)\n",
    "    df.loc[:, \"dropoff_lat_bin\"] = np.round(df[\"dropoff_latitude\"], precision)\n",
    "    df.loc[:, \"dropoff_long_bin\"] = np.round(df[\"dropoff_longitude\"], precision)\n",
    "\n",
    "bin_coordinates(df_train)\n",
    "bin_coordinates(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68d8d85c-1778-45fd-b4a4-03ec690bcbc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:21\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:16\u001b[0m, in \u001b[0;36mcompute_spatial_aggregations\u001b[1;34m(df, min_trips)\u001b[0m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\frame.py:9423\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9412\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9414\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9416\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9421\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9422\u001b[0m )\n\u001b[1;32m-> 9423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\apply.py:678\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\apply.py:798\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 798\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\apply.py:814\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 814\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    816\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m<timed exec>:16\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\base.py:780\u001b[0m, in \u001b[0;36mIndexOpsMixin.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;124;03mReturn an iterator of the values.\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03miterator\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;66;03m# We are explicitly making element iterators.\u001b[39;00m\n\u001b[1;32m--> 780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# Check type instead of dtype to catch DTA/TDA\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values)\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def compute_spatial_aggregations(df, min_trips=100):\n",
    "    \"\"\"Computes trip counts for different spatial aggregations.\"\"\"\n",
    "    groupings = [\n",
    "        [\"pickup_lat_bin\", \"pickup_long_bin\", \"dropoff_lat_bin\", \"dropoff_long_bin\"],\n",
    "        [\"pickup_lat_bin\", \"pickup_long_bin\"],\n",
    "        [\"dropoff_lat_bin\", \"dropoff_long_bin\"]\n",
    "    ]\n",
    "    \n",
    "    for groupby_cols in groupings:\n",
    "        col_name = \"cnt_coords_bin_\" + \"\".join(set([col[0] for col in groupby_cols]))\n",
    "        \n",
    "        # Compute trip counts and store in a dictionary for fast lookup\n",
    "        counts = df.groupby(groupby_cols).size().to_dict()\n",
    "        \n",
    "        # Apply counts to create a new column in the dataframe\n",
    "        df[col_name] = df[groupby_cols].apply(lambda row: counts.get(tuple(row), 0), axis=1)\n",
    "        \n",
    "        # Apply filtering based on min_trips\n",
    "        df[col_name] = df[col_name].where(df[col_name] >= min_trips, 0)\n",
    "\n",
    "compute_spatial_aggregations(df_train)\n",
    "compute_spatial_aggregations(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20859fd-b206-435a-8642-d67cc17f6a5a",
   "metadata": {},
   "source": [
    "- Temporal Aggregation of Trip Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5b93e364-02a3-4126-8748-7e0be72e30f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: sys\n",
      "Loading train.csv from: E:\\Term 2\\[CEGE0042] Spatial-Temporal Data Analysis & Mining\\Assignment\\Repository\\data\\train.csv\n",
      "Detected environment: sys\n",
      "Loading test.csv from: E:\\Term 2\\[CEGE0042] Spatial-Temporal Data Analysis & Mining\\Assignment\\Repository\\data\\test.csv\n",
      "CPU times: total: 5.77 s\n",
      "Wall time: 5.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the dataset\n",
    "df_train = load_data(\"train\")\n",
    "df_test = load_data(\"test\")\n",
    "\n",
    "bin_coordinates(df_train)\n",
    "bin_coordinates(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c825cc1-f516-468b-b9f8-5ae93ca1afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                        | 5813/1166915 [02:49<9:23:45, 34.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:78\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:49\u001b[0m, in \u001b[0;36mcompute_temporal_aggregation\u001b[1;34m(df, df_ref)\u001b[0m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\ops\\common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     79\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\arraylike.py:60\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\series.py:6096\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6093\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   6095\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 6096\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:279\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    271\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    272\u001b[0m         )\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    275\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    277\u001b[0m ):\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mne:\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\ops\\common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     79\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\arraylike.py:60\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Tools\\Anaconda3\\envs\\tf29-py38\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:977\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m    975\u001b[0m other_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbox(other)\n\u001b[0;32m    976\u001b[0m \u001b[38;5;66;03m# GH#37462 comparison on i8 values is almost 2x faster than M8/m8\u001b[39;00m\n\u001b[1;32m--> 977\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_vals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m o_mask \u001b[38;5;241m=\u001b[39m isna(other)\n\u001b[0;32m    980\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_isnan \u001b[38;5;241m|\u001b[39m o_mask\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def compute_temporal_aggregation(df, df_ref, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Compute temporal aggregated features for taxi trips\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to compute features for (must contain pickup/dropoff bins and timestamps)\n",
    "        df_ref: Reference DataFrame used for aggregation calculations\n",
    "    \n",
    "    Adds three new columns:\n",
    "        prev_1h_trip_count: Total trips in previous 1 hour window\n",
    "        mean_prev_3h_pickups_trip_count: Average hourly pickups in matching spatial bin (T-4h to T-1h)\n",
    "        mean_prev_3h_dropoffs_trip_count: Average hourly dropoffs in matching spatial bin (T-4h to T-1h)\n",
    "    \"\"\"\n",
    "    # Ensure pickup_datetime is in datetime format\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "    df_ref[\"pickup_datetime\"] = pd.to_datetime(df_ref[\"pickup_datetime\"])\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    tqdm.pandas(desc=\"Processing trips\")\n",
    "    \n",
    "    # Create temporary columns for time calculations\n",
    "    df_ref = df_ref.copy()\n",
    "    df_ref[\"pickup_hour\"] = df_ref[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "    \n",
    "    # Add new columns with default values\n",
    "    df[\"prev_1h_trip_count\"] = 0\n",
    "    df[\"mean_prev_3h_pickups_trip_count\"] = 0.0\n",
    "    df[\"mean_prev_3h_dropoffs_trip_count\"] = 0.0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Get current trip attributes\n",
    "        current_time = row[\"pickup_datetime\"]\n",
    "        current_hour = current_time.floor(\"H\")\n",
    "        pl_bin = row[\"pickup_lat_bin\"]\n",
    "        plon_bin = row[\"pickup_long_bin\"]\n",
    "        dl_bin = row[\"dropoff_lat_bin\"]\n",
    "        dlon_bin = row[\"dropoff_long_bin\"]\n",
    "        \n",
    "        # Calculate 1-hour window\n",
    "        t1_start = current_hour - pd.Timedelta(hours=1)\n",
    "        t1_end = current_hour\n",
    "        \n",
    "        # Calculate 3-hour average window (T-4h to T-1h)\n",
    "        t3_start = current_hour - pd.Timedelta(hours=4)\n",
    "        t3_end = current_hour - pd.Timedelta(hours=1)\n",
    "        \n",
    "        # Get reference data subsets\n",
    "        ref_1h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t1_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t1_end)\n",
    "        ]\n",
    "        \n",
    "        ref_3h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t3_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t3_end)\n",
    "        ]\n",
    "        \n",
    "        # Calculate 1-hour total count\n",
    "        df.at[idx, \"prev_1h_trip_count\"] = len(ref_1h)\n",
    "        \n",
    "        # Calculate 3-hour spatial averages\n",
    "        pickup_count = len(ref_3h[\n",
    "            (ref_3h[\"pickup_lat_bin\"] == pl_bin) &\n",
    "            (ref_3h[\"pickup_long_bin\"] == plon_bin)\n",
    "        ])\n",
    "        \n",
    "        dropoff_count = len(ref_3h[\n",
    "            (ref_3h[\"dropoff_lat_bin\"] == dl_bin) &\n",
    "            (ref_3h[\"dropoff_long_bin\"] == dlon_bin)\n",
    "        ])\n",
    "        \n",
    "        df.at[idx, \"mean_prev_3h_pickups_trip_count\"] = pickup_count / 3\n",
    "        df.at[idx, \"mean_prev_3h_dropoffs_trip_count\"] = dropoff_count / 3\n",
    "        \n",
    "    return df\n",
    "\n",
    "# For training data:\n",
    "df_train = compute_temporal_aggregation(df_train, df_train)\n",
    "\n",
    "# For test data:\n",
    "combined_ref = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_test = compute_temporal_aggregation(df_test, combined_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fb4b8-b9ae-4dde-aa83-ccf3b18bdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_chunk(chunk, df_ref):\n",
    "    \"\"\"\n",
    "    Process a chunk of the DataFrame to compute spatial-temporal features.\n",
    "    \n",
    "    Args:\n",
    "        chunk: A subset of the main DataFrame.\n",
    "        df_ref: Reference DataFrame for aggregation calculations.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed features for the chunk.\n",
    "    \"\"\"\n",
    "    # Ensure pickup_datetime is in datetime format\n",
    "    chunk[\"pickup_datetime\"] = pd.to_datetime(chunk[\"pickup_datetime\"])\n",
    "    df_ref[\"pickup_datetime\"] = pd.to_datetime(df_ref[\"pickup_datetime\"])\n",
    "    \n",
    "    # Create temporary columns for time calculations\n",
    "    df_ref = df_ref.copy()\n",
    "    df_ref[\"pickup_hour\"] = df_ref[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "    \n",
    "    # Add new columns with default values\n",
    "    chunk[\"prev_1h_trip_count\"] = 0\n",
    "    chunk[\"mean_prev_3h_pickups_trip_count\"] = 0.0\n",
    "    chunk[\"mean_prev_3h_dropoffs_trip_count\"] = 0.0\n",
    "    \n",
    "    for idx, row in chunk.iterrows():\n",
    "        # Get current trip attributes\n",
    "        current_time = row[\"pickup_datetime\"]\n",
    "        current_hour = current_time.floor(\"H\")\n",
    "        pl_bin = row[\"pickup_lat_bin\"]\n",
    "        plon_bin = row[\"pickup_long_bin\"]\n",
    "        dl_bin = row[\"dropoff_lat_bin\"]\n",
    "        dlon_bin = row[\"dropoff_long_bin\"]\n",
    "        \n",
    "        # Calculate 1-hour window\n",
    "        t1_start = current_hour - pd.Timedelta(hours=1)\n",
    "        t1_end = current_hour\n",
    "        \n",
    "        # Calculate 3-hour average window (T-4h to T-1h)\n",
    "        t3_start = current_hour - pd.Timedelta(hours=4)\n",
    "        t3_end = current_hour - pd.Timedelta(hours=1)\n",
    "        \n",
    "        # Get reference data subsets\n",
    "        ref_1h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t1_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t1_end)\n",
    "        ]\n",
    "        \n",
    "        ref_3h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t3_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t3_end)\n",
    "        ]\n",
    "        \n",
    "        # Calculate 1-hour total count\n",
    "        chunk.at[idx, \"prev_1h_trip_count\"] = len(ref_1h)\n",
    "        \n",
    "        # Calculate 3-hour spatial averages\n",
    "        pickup_count = len(ref_3h[\n",
    "            (ref_3h[\"pickup_lat_bin\"] == pl_bin) &\n",
    "            (ref_3h[\"pickup_long_bin\"] == plon_bin)\n",
    "        ])\n",
    "        \n",
    "        dropoff_count = len(ref_3h[\n",
    "            (ref_3h[\"dropoff_lat_bin\"] == dl_bin) &\n",
    "            (ref_3h[\"dropoff_long_bin\"] == dlon_bin)\n",
    "        ])\n",
    "        \n",
    "        chunk.at[idx, \"mean_prev_3h_pickups_trip_count\"] = pickup_count / 3\n",
    "        chunk.at[idx, \"mean_prev_3h_dropoffs_trip_count\"] = dropoff_count / 3\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "def compute_spatial_aggregation_parallel(df, df_ref, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Compute spatial-temporal aggregated features for taxi trips in parallel.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to compute features for (must contain pickup/dropoff bins and timestamps).\n",
    "        df_ref: Reference DataFrame used for aggregation calculations.\n",
    "        n_jobs: Number of parallel jobs to run. Default is -1 (use all available cores).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed features.\n",
    "    \"\"\"\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunks = [df.iloc[i:i + 1000] for i in range(0, len(df), 1000)]\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_chunk)(chunk, df_ref) for chunk in tqdm(chunks, desc=\"Processing chunks\")\n",
    "    )\n",
    "    \n",
    "    # Combine results into a single DataFrame\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# For training data:\n",
    "df_train = compute_temporal_aggregation(df_train, df_train)\n",
    "\n",
    "# For test data:\n",
    "combined_ref = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_test = compute_temporal_aggregation(df_test, combined_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea30a8c0-151d-41bf-ab40-3410f904cf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "于 pickup_datetime 字段，相等： 2016-03-10 15:17:28 == 2016-03-10 15:17:28\n",
      "于 mean_cnt_Tminus1h 字段，不相等： 322 != 318.0\n",
      "于 mean_cnt_pickup_trips_Tminus4h_Tminus1h 字段，不相等： 108 != 34.75\n",
      "于 mean_cnt_dropoff_trips_Tminus4h_Tminus1h 字段，不相等： 58 != 17.75\n",
      "于 pickup_datetime 字段，相等： 2016-02-20 08:58:35 == 2016-02-20 08:58:35\n",
      "于 mean_cnt_Tminus1h 字段，不相等： 115 != 152.0\n",
      "于 mean_cnt_pickup_trips_Tminus4h_Tminus1h 字段，不相等： 12 != 7.5\n",
      "于 mean_cnt_dropoff_trips_Tminus4h_Tminus1h 字段，不相等： 10 != 9.5\n",
      "于 pickup_datetime 字段，相等： 2016-05-10 15:08:15 == 2016-05-10 15:08:15\n",
      "于 mean_cnt_Tminus1h 字段，不相等： 310 != 314.0\n",
      "于 mean_cnt_pickup_trips_Tminus4h_Tminus1h 字段，不相等： 83 != 29.25\n",
      "于 mean_cnt_dropoff_trips_Tminus4h_Tminus1h 字段，不相等： 13 != 6.0\n",
      "          id     pickup_datetime  mean_cnt_Tminus1h  \\\n",
      "0  id3175894 2016-03-10 15:17:28                322   \n",
      "1  id2218059 2016-02-20 08:58:35                115   \n",
      "2  id3614707 2016-05-10 15:08:15                310   \n",
      "\n",
      "   mean_cnt_pickup_trips_Tminus4h_Tminus1h  \\\n",
      "0                                      108   \n",
      "1                                       12   \n",
      "2                                       83   \n",
      "\n",
      "   mean_cnt_dropoff_trips_Tminus4h_Tminus1h  \n",
      "0                                        58  \n",
      "1                                        10  \n",
      "2                                        13  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def check_trip_counts(df, k=3):\n",
    "    # 随机选择k=3条记录\n",
    "    selected_records = df.sample(n=k, random_state=42)\n",
    "    \n",
    "    # 用于保存检查结果的列表\n",
    "    results = []\n",
    "    \n",
    "    for idx, record in selected_records.iterrows():\n",
    "        pickup_datetime = record['pickup_datetime']\n",
    "        pickup_lat_bin = record['pickup_lat_bin']\n",
    "        pickup_long_bin = record['pickup_long_bin']\n",
    "        dropoff_lat_bin = record['dropoff_lat_bin']\n",
    "        dropoff_long_bin = record['dropoff_long_bin']\n",
    "        id_value = record['id']  # 获取id\n",
    "        \n",
    "        # 获取前1小时的时间窗口，向下取整到整小时\n",
    "        time_minus_1h_start = pd.to_datetime(pickup_datetime.replace(minute=0, second=0, microsecond=0)) - pd.Timedelta(hours=1)\n",
    "        time_minus_1h_end = pd.to_datetime(pickup_datetime.replace(minute=0, second=0, microsecond=0))\n",
    "        \n",
    "        # 获取前1小时范围内的记录并计算计数\n",
    "        count_Tminus1h = len(df[(df['pickup_datetime'] >= time_minus_1h_start) & \n",
    "                                (df['pickup_datetime'] < time_minus_1h_end)])\n",
    "        \n",
    "        # 检查前4小时到前1小时内，符合pickup条件的记录 (T-4h to T-1h)\n",
    "        time_minus_4h_start = pd.to_datetime(pickup_datetime.replace(minute=0, second=0, microsecond=0)) - pd.Timedelta(hours=4)\n",
    "        \n",
    "        # 获取前4小时到前1小时的记录并计算pickup条件下的计数\n",
    "        pickup_condition = df[(df['pickup_datetime'] >= time_minus_4h_start) & \n",
    "                              (df['pickup_datetime'] < time_minus_1h_end) & \n",
    "                              (df['pickup_lat_bin'] == pickup_lat_bin) & \n",
    "                              (df['pickup_long_bin'] == pickup_long_bin)]\n",
    "        count_pickup_Tminus4h_Tminus1h = len(pickup_condition)\n",
    "        \n",
    "        # 检查前4小时到前1小时内，符合dropoff条件的记录 (T-4h to T-1h)\n",
    "        dropoff_condition = df[(df['pickup_datetime'] >= time_minus_4h_start) & \n",
    "                               (df['pickup_datetime'] < time_minus_1h_end) & \n",
    "                               (df['dropoff_lat_bin'] == dropoff_lat_bin) & \n",
    "                               (df['dropoff_long_bin'] == dropoff_long_bin)]\n",
    "        count_dropoff_Tminus4h_Tminus1h = len(dropoff_condition)\n",
    "        \n",
    "        # 存储结果\n",
    "        results.append({\n",
    "            \"id\": id_value,  # 添加id\n",
    "            \"pickup_datetime\": pickup_datetime,\n",
    "            \"mean_cnt_Tminus1h\": count_Tminus1h,\n",
    "            \"mean_cnt_pickup_trips_Tminus4h_Tminus1h\": count_pickup_Tminus4h_Tminus1h,\n",
    "            \"mean_cnt_dropoff_trips_Tminus4h_Tminus1h\": count_dropoff_Tminus4h_Tminus1h\n",
    "        })\n",
    "    \n",
    "    # 转换为DataFrame返回\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # 检查每个元素和df中的记录是否相等\n",
    "    for idx, elem in result_df.iterrows():\n",
    "        # 根据id获取df中的记录\n",
    "        row = df[df['id'] == elem['id']].iloc[0]  # 获取df中id对应的记录\n",
    "        \n",
    "        # 比较results中的每个key与df中对应的列值是否相等\n",
    "        for key in elem.index:\n",
    "            if key != 'id':  # 排除id字段\n",
    "                if elem[key] == row[key]:\n",
    "                    print(f\"于 {key} 字段，相等： {elem[key]} == {row[key]}\")\n",
    "                else:\n",
    "                    print(f\"于 {key} 字段，不相等： {elem[key]} != {row[key]}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# 使用示例\n",
    "result_df = check_trip_counts(df_train, k=3)\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b56ba-7844-46ba-9ac0-fbddc3421355",
   "metadata": {},
   "source": [
    "### Drop Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e75d0b4d-aae2-4d44-80f8-96cb84a7ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pickup_timestamp', 'id', 'vendor_id', 'pickup_datetime',\n",
       "       'dropoff_datetime', 'passenger_count', 'pickup_longitude',\n",
       "       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
       "       'store_and_fwd_flag', 'trip_duration', 'cnt_trips_last_1h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f89404-2abf-4e2d-90d2-11296390dd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'vendor_id', 'pickup_datetime', 'dropoff_datetime',\n",
       "       'passenger_count', 'pickup_longitude', 'pickup_latitude',\n",
       "       'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag',\n",
       "       'trip_duration', 'pickup_pca0', 'pickup_pca1', 'dropoff_pca0',\n",
       "       'dropoff_pca1', 'euclidean_distance', 'pickup_hour_of_day',\n",
       "       'day_of_week', 'hour_of_week', 'month_of_year', 'day_of_year',\n",
       "       'week_of_year', 'hour_of_year', 'pickup_date', 'pickup_lat_bin',\n",
       "       'pickup_long_bin', 'dropoff_lat_bin', 'dropoff_long_bin',\n",
       "       'cnt_coords_bin_pd', 'cnt_coords_bin_p', 'cnt_coords_bin_d'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f507a63-4abe-4eb0-99d7-cb478c3dffa1",
   "metadata": {},
   "source": [
    "### Save Data in parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ff5b0-54a4-4492-8b0b-de0b3751b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Ensure the 'prep' directory exists\n",
    "os.makedirs(os.path.join(\"data\", \"prep\"), exist_ok=True)\n",
    "\n",
    "# Save to Parquet format\n",
    "df_train.to_parquet(\"prep/df_train.parquet\", index=False)\n",
    "df_test.to_parquet(\"prep/df_test.parquet\", index=False)\n",
    "\n",
    "print(\"df_train and df_test saved to 'prep' directory as Parquet files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea8ccf-0549-4236-a9fc-937cf4a11fe1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
