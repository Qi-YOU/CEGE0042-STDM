{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab2b8df-2342-462a-9c7b-f39c2821a259",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Trip Duration\n",
    "\n",
    "Generate code & functions such that conducts data preprocessing(includes feature engineering & data cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defb9d45-f563-46f0-b387-cd90597809b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 451 ms, total: 1.98 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import Data Handling Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import Date-Time Handling Libraries\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "\n",
    "# Import Geodetic Libraries\n",
    "import pyproj\n",
    "from pyproj import Geod\n",
    "\n",
    "# Import Data Visualization Libraries\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.size\"] = 12\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 12]  # Set default figure size\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Machine Learning Libraries\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Set random seed for reproducibility in scikit-learn\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "rng = check_random_state(42)\n",
    "\n",
    "# Import Utilities\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import json\n",
    "\n",
    "# Import Custom Modules\n",
    "from data_loader import *  # Custom data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15788607-720c-45b5-854c-8e4f409eb38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: sys\n",
      "Loading train.csv from: /root/CEGE0042-STDM/data/train.csv\n",
      "CPU times: user 2.44 s, sys: 586 ms, total: 3.02 s\n",
      "Wall time: 3.02 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id0458976</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-29 18:21:02</td>\n",
       "      <td>2016-06-29 18:39:55</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.862762</td>\n",
       "      <td>40.768822</td>\n",
       "      <td>-73.891701</td>\n",
       "      <td>40.746689</td>\n",
       "      <td>N</td>\n",
       "      <td>1133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id0434613</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-25 13:03:26</td>\n",
       "      <td>2016-04-25 13:18:13</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.958038</td>\n",
       "      <td>40.783237</td>\n",
       "      <td>-73.975510</td>\n",
       "      <td>40.760853</td>\n",
       "      <td>N</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3809234</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-07 12:36:09</td>\n",
       "      <td>2016-05-07 12:47:35</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.969460</td>\n",
       "      <td>40.785519</td>\n",
       "      <td>-73.989243</td>\n",
       "      <td>40.771748</td>\n",
       "      <td>N</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id1203705</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-14 18:44:17</td>\n",
       "      <td>2016-05-14 18:57:55</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.981743</td>\n",
       "      <td>40.736549</td>\n",
       "      <td>-73.998352</td>\n",
       "      <td>40.726440</td>\n",
       "      <td>N</td>\n",
       "      <td>818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id1896645</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-10 22:51:25</td>\n",
       "      <td>2016-04-10 23:07:16</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.977913</td>\n",
       "      <td>40.752609</td>\n",
       "      <td>-73.975647</td>\n",
       "      <td>40.733139</td>\n",
       "      <td>N</td>\n",
       "      <td>951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id0458976          2  2016-06-29 18:21:02  2016-06-29 18:39:55   \n",
       "1  id0434613          2  2016-04-25 13:03:26  2016-04-25 13:18:13   \n",
       "2  id3809234          2  2016-05-07 12:36:09  2016-05-07 12:47:35   \n",
       "3  id1203705          1  2016-05-14 18:44:17  2016-05-14 18:57:55   \n",
       "4  id1896645          2  2016-04-10 22:51:25  2016-04-10 23:07:16   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.862762        40.768822         -73.891701   \n",
       "1                1        -73.958038        40.783237         -73.975510   \n",
       "2                1        -73.969460        40.785519         -73.989243   \n",
       "3                1        -73.981743        40.736549         -73.998352   \n",
       "4                1        -73.977913        40.752609         -73.975647   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.746689                  N           1133  \n",
       "1         40.760853                  N            887  \n",
       "2         40.771748                  N            686  \n",
       "3         40.726440                  N            818  \n",
       "4         40.733139                  N            951  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the dataset\n",
    "df_train = load_data(\"train\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21c7161-4d10-4bb0-b02a-dc060bacce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1166915 entries, 0 to 1166914\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   id                  1166915 non-null  object \n",
      " 1   vendor_id           1166915 non-null  int64  \n",
      " 2   pickup_datetime     1166915 non-null  object \n",
      " 3   dropoff_datetime    1166915 non-null  object \n",
      " 4   passenger_count     1166915 non-null  int64  \n",
      " 5   pickup_longitude    1166915 non-null  float64\n",
      " 6   pickup_latitude     1166915 non-null  float64\n",
      " 7   dropoff_longitude   1166915 non-null  float64\n",
      " 8   dropoff_latitude    1166915 non-null  float64\n",
      " 9   store_and_fwd_flag  1166915 non-null  object \n",
      " 10  trip_duration       1166915 non-null  int64  \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 97.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8107f50a-eb4c-4dd8-8488-71e99a0650ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: sys\n",
      "Loading test.csv from: /root/CEGE0042-STDM/data/test.csv\n",
      "CPU times: user 610 ms, sys: 115 ms, total: 725 ms\n",
      "Wall time: 723 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2793718</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-08 07:36:19</td>\n",
       "      <td>2016-06-08 07:53:39</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.985611</td>\n",
       "      <td>40.735943</td>\n",
       "      <td>-73.980331</td>\n",
       "      <td>40.760468</td>\n",
       "      <td>N</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id3485529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-03 12:58:11</td>\n",
       "      <td>2016-04-03 13:11:58</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.978394</td>\n",
       "      <td>40.764351</td>\n",
       "      <td>-73.991623</td>\n",
       "      <td>40.749859</td>\n",
       "      <td>N</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1816614</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-06-05 02:49:13</td>\n",
       "      <td>2016-06-05 02:59:27</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.989059</td>\n",
       "      <td>40.744389</td>\n",
       "      <td>-73.973381</td>\n",
       "      <td>40.748692</td>\n",
       "      <td>N</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id1050851</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-05-05 17:18:27</td>\n",
       "      <td>2016-05-05 17:32:54</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.990326</td>\n",
       "      <td>40.731136</td>\n",
       "      <td>-73.991264</td>\n",
       "      <td>40.748917</td>\n",
       "      <td>N</td>\n",
       "      <td>867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id0140657</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-05-12 17:43:38</td>\n",
       "      <td>2016-05-12 19:06:25</td>\n",
       "      <td>4</td>\n",
       "      <td>-73.789497</td>\n",
       "      <td>40.646675</td>\n",
       "      <td>-73.987137</td>\n",
       "      <td>40.759232</td>\n",
       "      <td>N</td>\n",
       "      <td>4967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2793718          2  2016-06-08 07:36:19  2016-06-08 07:53:39   \n",
       "1  id3485529          2  2016-04-03 12:58:11  2016-04-03 13:11:58   \n",
       "2  id1816614          2  2016-06-05 02:49:13  2016-06-05 02:59:27   \n",
       "3  id1050851          2  2016-05-05 17:18:27  2016-05-05 17:32:54   \n",
       "4  id0140657          1  2016-05-12 17:43:38  2016-05-12 19:06:25   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.985611        40.735943         -73.980331   \n",
       "1                1        -73.978394        40.764351         -73.991623   \n",
       "2                5        -73.989059        40.744389         -73.973381   \n",
       "3                2        -73.990326        40.731136         -73.991264   \n",
       "4                4        -73.789497        40.646675         -73.987137   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.760468                  N           1040  \n",
       "1         40.749859                  N            827  \n",
       "2         40.748692                  N            614  \n",
       "3         40.748917                  N            867  \n",
       "4         40.759232                  N           4967  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the test dataset\n",
    "df_test = load_data(\"test\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c96dc95-4c16-48f0-8b2d-e7acf5d227ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 291729 entries, 0 to 291728\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   id                  291729 non-null  object \n",
      " 1   vendor_id           291729 non-null  int64  \n",
      " 2   pickup_datetime     291729 non-null  object \n",
      " 3   dropoff_datetime    291729 non-null  object \n",
      " 4   passenger_count     291729 non-null  int64  \n",
      " 5   pickup_longitude    291729 non-null  float64\n",
      " 6   pickup_latitude     291729 non-null  float64\n",
      " 7   dropoff_longitude   291729 non-null  float64\n",
      " 8   dropoff_latitude    291729 non-null  float64\n",
      " 9   store_and_fwd_flag  291729 non-null  object \n",
      " 10  trip_duration       291729 non-null  int64  \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 24.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34d95c2-ffb2-4507-a23f-bd072c4ba113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61 ms, sys: 31.3 ms, total: 92.3 ms\n",
      "Wall time: 90.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Delete cols that leads to data leakage\n",
    "del df_train[\"dropoff_datetime\"]\n",
    "del df_test[\"dropoff_datetime\"]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f979079-f2a5-4b2e-a991-f70b65f5e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 13.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define helper function formats time seconds into string\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours} hour {minutes} min {seconds:.2f} sec\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b1bff-dd6d-4481-9a9b-473b2815cbb0",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71558b23-63ca-4c2e-88f7-7f91270f0ce5",
   "metadata": {},
   "source": [
    "### PCA in Longitudes & Latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75e8fa9-b231-47ee-b54b-7557f0d3b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 885 ms, sys: 851 ms, total: 1.74 s\n",
      "Wall time: 341 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def apply_pca_to_coords(train, test, random_seed=42):\n",
    "    \"\"\"\n",
    "    Applies PCA transformation to pickup and dropoff coordinates for train and test datasets.\n",
    "\n",
    "    The PCA is fitted **only on the training data** to prevent data leakage.\n",
    "\n",
    "    Parameters:\n",
    "        train (pd.DataFrame): The training dataset.\n",
    "        test (pd.DataFrame): The testing dataset.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies train and test DataFrames in place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit PCA only on training data\n",
    "    coords_train = np.vstack((\n",
    "        train[[\"pickup_latitude\", \"pickup_longitude\"]].values,\n",
    "        train[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    ))\n",
    "\n",
    "    pca = PCA(whiten=True, random_state=random_seed).fit(coords_train)\n",
    "\n",
    "    # Apply transformation to train dataset\n",
    "    train_coords_pickup = train[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "    train_coords_dropoff = train[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    train.loc[:, \"pickup_pca0\"] = pca.transform(train_coords_pickup)[:, 0]\n",
    "    train.loc[:, \"pickup_pca1\"] = pca.transform(train_coords_pickup)[:, 1]\n",
    "    train.loc[:, \"dropoff_pca0\"] = pca.transform(train_coords_dropoff)[:, 0]\n",
    "    train.loc[:, \"dropoff_pca1\"] = pca.transform(train_coords_dropoff)[:, 1]\n",
    "\n",
    "    # Apply the same transformation to test dataset to avoid data leakage\n",
    "    test_coords_pickup = test[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "    test_coords_dropoff = test[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    test.loc[:, \"pickup_pca0\"] = pca.transform(test_coords_pickup)[:, 0]\n",
    "    test.loc[:, \"pickup_pca1\"] = pca.transform(test_coords_pickup)[:, 1]\n",
    "    test.loc[:, \"dropoff_pca0\"] = pca.transform(test_coords_dropoff)[:, 0]\n",
    "    test.loc[:, \"dropoff_pca1\"] = pca.transform(test_coords_dropoff)[:, 1]\n",
    "\n",
    "# Example usage:\n",
    "apply_pca_to_coords(df_train, df_test, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814f521-b108-4543-91a0-5095f831fb1c",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7a9992-3bcc-422a-9c7d-8318cbaa6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 787 ms, total: 17.1 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define WGS84 ellipsoid\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Compute great-circle distance in kilometers\n",
    "df_train[\"euclidean_distance\"] = df_train.apply(\n",
    "    lambda row: geod.inv(row[\"pickup_longitude\"], row[\"pickup_latitude\"],\n",
    "                         row[\"dropoff_longitude\"], row[\"dropoff_latitude\"])[2] / 1000, axis=1\n",
    ")\n",
    "\n",
    "# Compute great-circle distance in kilometers\n",
    "df_test[\"euclidean_distance\"] = df_test.apply(\n",
    "    lambda row: geod.inv(row[\"pickup_longitude\"], row[\"pickup_latitude\"],\n",
    "                         row[\"dropoff_longitude\"], row[\"dropoff_latitude\"])[2] / 1000, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c2049-3816-4d41-9718-fdd59f412c65",
   "metadata": {},
   "source": [
    "### Datetime Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c665f53-2be9-4270-b4a3-bada27f573e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 s, sys: 23.7 ms, total: 1.15 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generate_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Generate detailed date-time features for pickups and modify the DataFrame in place.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the datetime column.\n",
    "    \n",
    "    Returns:\n",
    "        None (Modifies df in place)\n",
    "    \"\"\"\n",
    "    # Convert to datetime format\n",
    "    pickup_times = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "\n",
    "    # Extract relevant time features as integers\n",
    "    df[\"pickup_hour_of_day\"] = (pickup_times.dt.hour * 60 + pickup_times.dt.minute) // 60  # Integer division\n",
    "\n",
    "    df[\"day_of_week\"] = pickup_times.dt.weekday.astype(int)\n",
    "    df[\"hour_of_week\"] = (df[\"day_of_week\"] * 24 + df[\"pickup_hour_of_day\"]).astype(int)\n",
    "\n",
    "    df[\"month_of_year\"] = pickup_times.dt.month.astype(int)\n",
    "    df[\"day_of_year\"] = pickup_times.dt.dayofyear.astype(int)\n",
    "    df[\"week_of_year\"] = pickup_times.dt.isocalendar().week.astype(int)\n",
    "    df[\"hour_of_year\"] = (df[\"day_of_year\"] * 24 + df[\"pickup_hour_of_day\"]).astype(int)\n",
    "\n",
    "generate_datetime_features(df_train)\n",
    "generate_datetime_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729d5e0-34b6-41db-a562-fa3bfa155c86",
   "metadata": {},
   "source": [
    "### NYC Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56df2219-78a3-4ddd-bcb6-ae3701e5c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 236 ms, total: 1.24 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def merge_weather_data(df):\n",
    "    \"\"\"\n",
    "    Merges weather data with a given dataframe (train or test) based on the pickup date.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The train or test dataframe containing 'pickup_datetime'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The merged dataframe with only the intermediate weather data columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load NYC weather data to enrich information\n",
    "    weather_data = pd.read_csv(os.path.join(\"utils\", \"weather_data_nyc_centralpark_2016.csv\"), low_memory=False)\n",
    "    weather_data[\"date\"] = pd.to_datetime(weather_data[\"date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Ensure datetime consistency\n",
    "    weather_data[\"date\"] = weather_data[\"date\"].dt.date\n",
    "    df[\"pickup_date\"] = pd.to_datetime(df[\"pickup_datetime\"]).dt.date\n",
    "\n",
    "    # Handle trace values in precipitation, snow fall, and snow depth columns\n",
    "    weather_data[\"r_depth\"] = weather_data[\"precipitation\"].apply(lambda x: 0.01 if x == \"T\" else float(x))  # rain depth\n",
    "    weather_data[\"s_fall\"] = weather_data[\"snow fall\"].apply(lambda x: 0.01 if x == \"T\" else float(x))  # snow fall\n",
    "    weather_data[\"s_depth\"] = weather_data[\"snow depth\"].apply(lambda x: 0.01 if x == \"T\" else float(x))  # snow depth\n",
    "\n",
    "    # Calculate total precipitation, and snow/rain indicators\n",
    "    weather_data[\"all_precip\"] = weather_data[\"s_fall\"] + weather_data[\"r_depth\"]\n",
    "    weather_data[\"has_snow\"] = (weather_data[\"s_fall\"] > 0) | (weather_data[\"s_depth\"] > 0)\n",
    "    weather_data[\"has_rain\"] = weather_data[\"r_depth\"] > 0\n",
    "\n",
    "    # Copy temperature columns\n",
    "    weather_data[\"max_temp\"] = weather_data[\"maximum temperature\"]\n",
    "    weather_data[\"min_temp\"] = weather_data[\"minimum temperature\"]\n",
    "\n",
    "    # Select only the newly created columns\n",
    "    weather_data = weather_data[[\"date\", \"r_depth\", \"s_fall\", \"s_depth\", \"all_precip\", \"has_snow\", \"has_rain\", \"max_temp\", \"min_temp\"]]\n",
    "\n",
    "    # Merge the datasets on the date\n",
    "    df = df.merge(weather_data, left_on=\"pickup_date\", right_on=\"date\", how=\"left\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "df_train = merge_weather_data(df_train)\n",
    "df_test = merge_weather_data(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10577d-68e2-44e6-9a4b-191c14a6e0c0",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c71eaa-60cb-4292-83b1-af4296aeb41f",
   "metadata": {},
   "source": [
    "### Location Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a12284ae-e52d-453c-9129-0d4d3aa5c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before filtering: 1166915\n",
      "Records after filtering: 1166864\n",
      "Records dropped: 51\n",
      "\n",
      "Records before filtering: 291729\n",
      "Records after filtering: 291713\n",
      "Records dropped: 16\n",
      "\n",
      "CPU times: user 458 ms, sys: 116 ms, total: 574 ms\n",
      "Wall time: 571 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_nyc_boundary(df, geojson_path):\n",
    "    \"\"\"\n",
    "    Filters pickup and dropoff locations to keep only those within the New York City boundary.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing pickup and dropoff coordinates.\n",
    "        geojson_path (str): Path to the GeoJSON file defining NYC boundaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with locations inside the NYC bounding box.\n",
    "    \"\"\"\n",
    "    # Load the GeoJSON file\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    # Extract NYC boundary coordinates where NAME is \"New York\"\n",
    "    nyc_coords = []\n",
    "    for feature in geojson_data[\"features\"]:\n",
    "        if feature[\"properties\"].get(\"NAME\") == \"New York\":\n",
    "            for polygon in feature[\"geometry\"][\"coordinates\"]:  # Loop through MultiPolygon\n",
    "                for ring in polygon:  # Each polygon has a ring of coordinates\n",
    "                    nyc_coords.extend(ring)\n",
    "\n",
    "    # Compute NYC bounding box (min/max latitudes & longitudes)\n",
    "    min_long = min(lon for lon, lat in nyc_coords)\n",
    "    max_long = max(lon for lon, lat in nyc_coords)\n",
    "    min_lat = min(lat for lon, lat in nyc_coords)\n",
    "    max_lat = max(lat for lon, lat in nyc_coords)\n",
    "\n",
    "    # Count records before filtering\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Filter data based on bounding box\n",
    "    mask = (\n",
    "        (df[\"pickup_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"pickup_latitude\"].between(min_lat, max_lat))\n",
    "        & (df[\"dropoff_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"dropoff_latitude\"].between(min_lat, max_lat))\n",
    "    )\n",
    "\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    # Count records after filtering\n",
    "    final_count = len(filtered_df)\n",
    "    dropped_count = initial_count - final_count\n",
    "\n",
    "    print(f\"Records before filtering: {initial_count}\")\n",
    "    print(f\"Records after filtering: {final_count}\")\n",
    "    print(f\"Records dropped: {dropped_count}\\n\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Apply function to df_train and df_test\n",
    "df_train = filter_by_nyc_boundary(df_train, \"utils/gz_2010_us_040_00_5m.json\")\n",
    "df_test = filter_by_nyc_boundary(df_test, \"utils/gz_2010_us_040_00_5m.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b448cff-b9fa-4483-bbb1-92e28066cfb7",
   "metadata": {},
   "source": [
    "### Distance & Duration Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d18fa224-e2c7-4ecf-9650-11735c5fff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before filtering (euclidean_distance): 1166864\n",
      "Records after filtering (euclidean_distance): 1165113\n",
      "Records dropped due to percentile limits (euclidean_distance): 1751\n",
      "Applied percentile limits (euclidean_distance): 0.0000 - 23.3741\n",
      "Applied lower bound after filtering (euclidean_distance): 0.1\n",
      "Total records dropped due to euclidean_distance outliers: 12369\n",
      "\n",
      "Records before filtering (trip_duration): 1154495\n",
      "Records after filtering (trip_duration): 1151066\n",
      "Records dropped due to percentile limits (trip_duration): 3429\n",
      "Applied percentile limits (trip_duration): 54.0000 - 7331.2590\n",
      "Applied lower bound after filtering (trip_duration): 300\n",
      "Total records dropped due to trip_duration outliers: 172332\n",
      "CPU times: user 797 ms, sys: 176 ms, total: 973 ms\n",
      "Wall time: 969 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_range(df, column, percentile_threshold=0.15, lower_bound=None, upper_bound=None):\n",
    "    \"\"\"\n",
    "    Filters trips based on a given column (e.g., Euclidean distance or trip duration) \n",
    "    using percentile thresholds and absolute bounds.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the column to filter.\n",
    "        column (str): The column to apply filtering on (e.g., 'euclidean_distance', 'trip_duration').\n",
    "        percentile_threshold (float, optional): The percentile threshold to remove outliers.\n",
    "                                                Defaults to 0.15 (removes 0.15% lowest and highest values).\n",
    "        lower_bound (float, optional): Absolute minimum value to keep. Defaults to None (not applied).\n",
    "        upper_bound (float, optional): Absolute maximum value to keep. Defaults to None (not applied).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with values within the specified range.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "    \n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Calculate percentile-based limits\n",
    "    lower_lim = np.percentile(df[column], percentile_threshold)\n",
    "    upper_lim = np.percentile(df[column], 100 - percentile_threshold)\n",
    "\n",
    "    # Apply filtering\n",
    "    filtered_df = df[df[column].between(lower_lim, upper_lim)]\n",
    "\n",
    "    filtered_count = len(filtered_df)\n",
    "    dropped_count = initial_count - filtered_count\n",
    "\n",
    "    print(f\"Records before filtering ({column}): {initial_count}\")\n",
    "    print(f\"Records after filtering ({column}): {filtered_count}\")\n",
    "    print(f\"Records dropped due to percentile limits ({column}): {dropped_count}\")\n",
    "    print(f\"Applied percentile limits ({column}): {lower_lim:.4f} - {upper_lim:.4f}\")\n",
    "    \n",
    "    if lower_bound is not None:\n",
    "        filtered_df = filtered_df[filtered_df[column] > lower_bound]\n",
    "        print(f\"Applied lower bound after filtering ({column}): {lower_bound}\")\n",
    "    if upper_bound is not None:\n",
    "        filtered_df = filtered_df[filtered_df[column] < upper_bound]\n",
    "        print(f\"Applied upper bound after filtering ({column}): {upper_bound}\")\n",
    "        \n",
    "    final_count = len(filtered_df)\n",
    "    dropped_count = initial_count - final_count\n",
    "    print(f\"Total records dropped due to {column} outliers: {dropped_count}\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Apply function to filter both euclidean_distance and trip_duration\n",
    "df_train = filter_by_range(df_train, \"euclidean_distance\", 0.15, lower_bound=0.1)\n",
    "print()\n",
    "df_train = filter_by_range(df_train, \"trip_duration\", 0.15, lower_bound=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00651e3-34a4-4264-8012-d7b59958eda4",
   "metadata": {},
   "source": [
    "### Spatial & Temporal Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df0882b0-18ae-4385-83ff-d5099f2da1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.4 ms, sys: 329 µs, total: 26.8 ms\n",
      "Wall time: 25.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def bin_coordinates(df, precision=2):\n",
    "    \"\"\"Bins latitude and longitude to a specified precision.\"\"\"\n",
    "    df.loc[:, \"pickup_lat_bin\"] = np.round(df[\"pickup_latitude\"], precision)\n",
    "    df.loc[:, \"pickup_long_bin\"] = np.round(df[\"pickup_longitude\"], precision)\n",
    "    df.loc[:, \"dropoff_lat_bin\"] = np.round(df[\"dropoff_latitude\"], precision)\n",
    "    df.loc[:, \"dropoff_long_bin\"] = np.round(df[\"dropoff_longitude\"], precision)\n",
    "\n",
    "bin_coordinates(df_train)\n",
    "bin_coordinates(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68d8d85c-1778-45fd-b4a4-03ec690bcbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.4 s, sys: 111 ms, total: 17.5 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def compute_spatial_aggregations(df, min_trips=100):\n",
    "    \"\"\"Computes trip counts for different spatial aggregations.\"\"\"\n",
    "    groupings = [\n",
    "        [\"pickup_lat_bin\", \"pickup_long_bin\", \"dropoff_lat_bin\", \"dropoff_long_bin\"],\n",
    "        [\"pickup_lat_bin\", \"pickup_long_bin\"],\n",
    "        [\"dropoff_lat_bin\", \"dropoff_long_bin\"]\n",
    "    ]\n",
    "    \n",
    "    for groupby_cols in groupings:\n",
    "        col_name = \"cnt_coords_bin_\" + \"\".join(set([col[0] for col in groupby_cols]))\n",
    "        \n",
    "        # Compute trip counts and store in a dictionary for fast lookup\n",
    "        counts = df.groupby(groupby_cols).size().to_dict()\n",
    "        \n",
    "        # Apply counts to create a new column in the dataframe\n",
    "        df[col_name] = df[groupby_cols].apply(lambda row: counts.get(tuple(row), 0), axis=1)\n",
    "        \n",
    "        # Apply filtering based on min_trips\n",
    "        df[col_name] = df[col_name].where(df[col_name] >= min_trips, 0)\n",
    "\n",
    "compute_spatial_aggregations(df_train)\n",
    "compute_spatial_aggregations(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf0fb4b8-b9ae-4dde-aa83-ccf3b18bdf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 13.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_chunk(chunk, df_ref):\n",
    "    \"\"\"\n",
    "    Process a chunk of the DataFrame to compute spatial-temporal features.\n",
    "    \n",
    "    Args:\n",
    "        chunk: A subset of the main DataFrame.\n",
    "        df_ref: Reference DataFrame for aggregation calculations.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed features for the chunk.\n",
    "    \"\"\"\n",
    "    # Ensure pickup_datetime is in datetime format\n",
    "    chunk[\"pickup_datetime\"] = pd.to_datetime(chunk[\"pickup_datetime\"])\n",
    "    df_ref[\"pickup_datetime\"] = pd.to_datetime(df_ref[\"pickup_datetime\"])\n",
    "    \n",
    "    # Create temporary columns for time calculations\n",
    "    df_ref = df_ref.copy()\n",
    "    df_ref[\"pickup_hour\"] = df_ref[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "    \n",
    "    # Add new columns with default values\n",
    "    chunk[\"cnt_prev_1h\"] = 0\n",
    "    chunk[\"cnt_mean_prev_3h_pickups\"] = 0.0\n",
    "    chunk[\"cnt_mean_prev_3h_dropoffs\"] = 0.0\n",
    "    \n",
    "    for idx, row in chunk.iterrows():\n",
    "        # Get current trip attributes\n",
    "        current_time = row[\"pickup_datetime\"]\n",
    "        current_hour = current_time.floor(\"H\")\n",
    "        pl_bin = row[\"pickup_lat_bin\"]\n",
    "        plon_bin = row[\"pickup_long_bin\"]\n",
    "        dl_bin = row[\"dropoff_lat_bin\"]\n",
    "        dlon_bin = row[\"dropoff_long_bin\"]\n",
    "        \n",
    "        # Calculate 1-hour window\n",
    "        t1_start = current_hour - pd.Timedelta(hours=1)\n",
    "        t1_end = current_hour\n",
    "        \n",
    "        # Calculate 3-hour average window (T-4h to T-1h)\n",
    "        t3_start = current_hour - pd.Timedelta(hours=4)\n",
    "        t3_end = current_hour - pd.Timedelta(hours=1)\n",
    "        \n",
    "        # Get reference data subsets\n",
    "        ref_1h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t1_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t1_end)\n",
    "        ]\n",
    "        \n",
    "        ref_3h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t3_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t3_end)\n",
    "        ]\n",
    "        \n",
    "        # Calculate 1-hour total count\n",
    "        chunk.at[idx, \"cnt_prev_1h\"] = len(ref_1h)\n",
    "        \n",
    "        # Calculate 3-hour spatial averages\n",
    "        pickup_count = len(ref_3h[\n",
    "            (ref_3h[\"pickup_lat_bin\"] == pl_bin) &\n",
    "            (ref_3h[\"pickup_long_bin\"] == plon_bin)\n",
    "        ])\n",
    "        \n",
    "        dropoff_count = len(ref_3h[\n",
    "            (ref_3h[\"dropoff_lat_bin\"] == dl_bin) &\n",
    "            (ref_3h[\"dropoff_long_bin\"] == dlon_bin)\n",
    "        ])\n",
    "        \n",
    "        chunk.at[idx, \"cnt_mean_prev_3h_pickups\"] = pickup_count / 3\n",
    "        chunk.at[idx, \"cnt_mean_prev_3h_dropoffs\"] = dropoff_count / 3\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "def compute_spatial_temporal_aggregation_parallel(df, df_ref, n_jobs):\n",
    "    \"\"\"\n",
    "    Compute spatial-temporal aggregated features for taxi trips in parallel.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to compute features for (must contain pickup/dropoff bins and timestamps).\n",
    "        df_ref: Reference DataFrame used for aggregation calculations.\n",
    "        n_jobs: Number of parallel jobs to run. Default is -1 (use all available cores).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine chunk size based on available cores\n",
    "    num_chunks = min(n_jobs, len(df))\n",
    "    chunk_size = len(df) // num_chunks if num_chunks > 0 else len(df)\n",
    "    print(f\"Available CPU Core: {n_jobs} | Chunk Size: {chunk_size}\")\n",
    "    \n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_chunk)(chunk, df_ref) for chunk in chunks\n",
    "    )\n",
    "    \n",
    "    # Combine results into a single DataFrame\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74a572f3-82ee-4677-906a-9d34a9ce85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU Core: 8 | Chunk Size: 122770\n",
      "CPU times: user 28.3 s, sys: 13.5 s, total: 41.8 s\n",
      "Wall time: 35min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For training data:\n",
    "df_train = compute_spatial_temporal_aggregation_parallel(df_train, df_train, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac4a9c0-1720-4041-a84d-578345ccd8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU Core: 8 | Chunk Size: 36464\n",
      "CPU times: user 38.9 s, sys: 10.6 s, total: 49.5 s\n",
      "Wall time: 13min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# For test data:\n",
    "combined_ref = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_test = compute_spatial_temporal_aggregation_parallel(df_test, combined_ref, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b56ba-7844-46ba-9ac0-fbddc3421355",
   "metadata": {},
   "source": [
    "### Drop Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e75d0b4d-aae2-4d44-80f8-96cb84a7ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'vendor_id', 'pickup_datetime', 'passenger_count',\n",
       "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
       "       'dropoff_latitude', 'store_and_fwd_flag', 'trip_duration',\n",
       "       'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1',\n",
       "       'euclidean_distance', 'pickup_hour_of_day', 'day_of_week',\n",
       "       'hour_of_week', 'month_of_year', 'day_of_year', 'week_of_year',\n",
       "       'hour_of_year', 'pickup_date', 'date', 'r_depth', 's_fall', 's_depth',\n",
       "       'all_precip', 'has_snow', 'has_rain', 'max_temp', 'min_temp',\n",
       "       'pickup_lat_bin', 'pickup_long_bin', 'dropoff_lat_bin',\n",
       "       'dropoff_long_bin', 'cnt_coords_bin_dp', 'cnt_coords_bin_p',\n",
       "       'cnt_coords_bin_d', 'cnt_prev_1h', 'cnt_mean_prev_3h_pickups',\n",
       "       'cnt_mean_prev_3h_dropoffs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "654d2eb3-6aed-4188-8b43-e192c00aa5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 532 ms, sys: 360 ms, total: 892 ms\n",
      "Wall time: 891 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'vendor_id', 'passenger_count', 'store_and_fwd_flag',\n",
       "       'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1',\n",
       "       'euclidean_distance', 'pickup_hour_of_day', 'day_of_week',\n",
       "       'hour_of_week', 'month_of_year', 'day_of_year', 'week_of_year',\n",
       "       'hour_of_year', 'r_depth', 's_fall', 's_depth', 'all_precip',\n",
       "       'has_snow', 'has_rain', 'max_temp', 'min_temp', 'cnt_coords_bin_dp',\n",
       "       'cnt_coords_bin_p', 'cnt_coords_bin_d', 'cnt_prev_1h',\n",
       "       'cnt_mean_prev_3h_pickups', 'cnt_mean_prev_3h_dropoffs',\n",
       "       'trip_duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Delete redundant, intermediate columns\n",
    "df_train.drop(columns=[\n",
    "    \"pickup_datetime\", \"pickup_date\", \"date\",\n",
    "    \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "    \"pickup_lat_bin\", \"pickup_long_bin\", \"dropoff_lat_bin\", \"dropoff_long_bin\"\n",
    "], inplace=True)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Reorganize the columns to make `trip_duration` the target column in the end\n",
    "df_train = df_train[[col for col in df_train.columns if col != \"trip_duration\"] + [\"trip_duration\"]]\n",
    "\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3972ea58-f46a-428e-ae1e-61c3c126cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 982163 entries, 0 to 982162\n",
      "Data columns (total 31 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   id                         982163 non-null  object \n",
      " 1   vendor_id                  982163 non-null  int64  \n",
      " 2   passenger_count            982163 non-null  int64  \n",
      " 3   store_and_fwd_flag         982163 non-null  object \n",
      " 4   pickup_pca0                982163 non-null  float64\n",
      " 5   pickup_pca1                982163 non-null  float64\n",
      " 6   dropoff_pca0               982163 non-null  float64\n",
      " 7   dropoff_pca1               982163 non-null  float64\n",
      " 8   euclidean_distance         982163 non-null  float64\n",
      " 9   pickup_hour_of_day         982163 non-null  int32  \n",
      " 10  day_of_week                982163 non-null  int64  \n",
      " 11  hour_of_week               982163 non-null  int64  \n",
      " 12  month_of_year              982163 non-null  int64  \n",
      " 13  day_of_year                982163 non-null  int64  \n",
      " 14  week_of_year               982163 non-null  int64  \n",
      " 15  hour_of_year               982163 non-null  int64  \n",
      " 16  r_depth                    982163 non-null  float64\n",
      " 17  s_fall                     982163 non-null  float64\n",
      " 18  s_depth                    982163 non-null  float64\n",
      " 19  all_precip                 982163 non-null  float64\n",
      " 20  has_snow                   982163 non-null  bool   \n",
      " 21  has_rain                   982163 non-null  bool   \n",
      " 22  max_temp                   982163 non-null  int64  \n",
      " 23  min_temp                   982163 non-null  int64  \n",
      " 24  cnt_coords_bin_dp          982163 non-null  int64  \n",
      " 25  cnt_coords_bin_p           982163 non-null  int64  \n",
      " 26  cnt_coords_bin_d           982163 non-null  int64  \n",
      " 27  cnt_prev_1h                982163 non-null  int64  \n",
      " 28  cnt_mean_prev_3h_pickups   982163 non-null  float64\n",
      " 29  cnt_mean_prev_3h_dropoffs  982163 non-null  float64\n",
      " 30  trip_duration              982163 non-null  int64  \n",
      "dtypes: bool(2), float64(11), int32(1), int64(15), object(2)\n",
      "memory usage: 215.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feb58e6a-b540-44f1-9783-1cab1fed3c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_pca0</th>\n",
       "      <th>pickup_pca1</th>\n",
       "      <th>dropoff_pca0</th>\n",
       "      <th>dropoff_pca1</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>pickup_hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>all_precip</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>cnt_coords_bin_dp</th>\n",
       "      <th>cnt_coords_bin_p</th>\n",
       "      <th>cnt_coords_bin_d</th>\n",
       "      <th>cnt_prev_1h</th>\n",
       "      <th>cnt_mean_prev_3h_pickups</th>\n",
       "      <th>cnt_mean_prev_3h_dropoffs</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "      <td>982163.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.534791</td>\n",
       "      <td>1.669296</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.039290</td>\n",
       "      <td>-0.003674</td>\n",
       "      <td>-0.019589</td>\n",
       "      <td>3.859115</td>\n",
       "      <td>13.700199</td>\n",
       "      <td>3.036553</td>\n",
       "      <td>86.577471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146849</td>\n",
       "      <td>59.544171</td>\n",
       "      <td>43.748533</td>\n",
       "      <td>968.380960</td>\n",
       "      <td>30085.929062</td>\n",
       "      <td>23902.928276</td>\n",
       "      <td>270.125312</td>\n",
       "      <td>8.146983</td>\n",
       "      <td>6.537344</td>\n",
       "      <td>945.804442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498788</td>\n",
       "      <td>1.314079</td>\n",
       "      <td>0.513579</td>\n",
       "      <td>0.823351</td>\n",
       "      <td>0.480059</td>\n",
       "      <td>0.939661</td>\n",
       "      <td>3.967002</td>\n",
       "      <td>6.351028</td>\n",
       "      <td>1.941386</td>\n",
       "      <td>46.491919</td>\n",
       "      <td>...</td>\n",
       "      <td>1.050329</td>\n",
       "      <td>16.925423</td>\n",
       "      <td>14.797519</td>\n",
       "      <td>1081.646917</td>\n",
       "      <td>18835.605582</td>\n",
       "      <td>17550.652579</td>\n",
       "      <td>81.562234</td>\n",
       "      <td>6.869240</td>\n",
       "      <td>6.392805</td>\n",
       "      <td>641.938199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.617779</td>\n",
       "      <td>-7.299590</td>\n",
       "      <td>-5.926344</td>\n",
       "      <td>-7.232434</td>\n",
       "      <td>0.100036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>301.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.079142</td>\n",
       "      <td>-0.405583</td>\n",
       "      <td>-0.141495</td>\n",
       "      <td>-0.468246</td>\n",
       "      <td>1.547194</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>15864.000000</td>\n",
       "      <td>9226.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.106446</td>\n",
       "      <td>0.059886</td>\n",
       "      <td>0.083917</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>2.460965</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>685.000000</td>\n",
       "      <td>27178.000000</td>\n",
       "      <td>21705.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>760.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.252120</td>\n",
       "      <td>0.424876</td>\n",
       "      <td>0.241034</td>\n",
       "      <td>0.472858</td>\n",
       "      <td>4.407869</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>1357.000000</td>\n",
       "      <td>45476.000000</td>\n",
       "      <td>35690.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.688035</td>\n",
       "      <td>11.410693</td>\n",
       "      <td>4.104426</td>\n",
       "      <td>9.376627</td>\n",
       "      <td>23.370427</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29.610000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>5687.000000</td>\n",
       "      <td>62262.000000</td>\n",
       "      <td>54389.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.333333</td>\n",
       "      <td>7331.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           vendor_id  passenger_count    pickup_pca0    pickup_pca1  \\\n",
       "count  982163.000000    982163.000000  982163.000000  982163.000000   \n",
       "mean        1.534791         1.669296      -0.001696      -0.039290   \n",
       "std         0.498788         1.314079       0.513579       0.823351   \n",
       "min         1.000000         0.000000      -5.617779      -7.299590   \n",
       "25%         1.000000         1.000000      -0.079142      -0.405583   \n",
       "50%         2.000000         1.000000       0.106446       0.059886   \n",
       "75%         2.000000         2.000000       0.252120       0.424876   \n",
       "max         2.000000         6.000000       4.688035      11.410693   \n",
       "\n",
       "        dropoff_pca0   dropoff_pca1  euclidean_distance  pickup_hour_of_day  \\\n",
       "count  982163.000000  982163.000000       982163.000000       982163.000000   \n",
       "mean       -0.003674      -0.019589            3.859115           13.700199   \n",
       "std         0.480059       0.939661            3.967002            6.351028   \n",
       "min        -5.926344      -7.232434            0.100036            0.000000   \n",
       "25%        -0.141495      -0.468246            1.547194            9.000000   \n",
       "50%         0.083917       0.070701            2.460965           14.000000   \n",
       "75%         0.241034       0.472858            4.407869           19.000000   \n",
       "max         4.104426       9.376627           23.370427           23.000000   \n",
       "\n",
       "         day_of_week   hour_of_week  ...     all_precip       max_temp  \\\n",
       "count  982163.000000  982163.000000  ...  982163.000000  982163.000000   \n",
       "mean        3.036553      86.577471  ...       0.146849      59.544171   \n",
       "std         1.941386      46.491919  ...       1.050329      16.925423   \n",
       "min         0.000000       0.000000  ...       0.000000      15.000000   \n",
       "25%         1.000000      45.000000  ...       0.000000      46.000000   \n",
       "50%         3.000000      88.000000  ...       0.000000      59.000000   \n",
       "75%         5.000000     126.000000  ...       0.040000      73.000000   \n",
       "max         6.000000     167.000000  ...      29.610000      92.000000   \n",
       "\n",
       "            min_temp  cnt_coords_bin_dp  cnt_coords_bin_p  cnt_coords_bin_d  \\\n",
       "count  982163.000000      982163.000000     982163.000000     982163.000000   \n",
       "mean       43.748533         968.380960      30085.929062      23902.928276   \n",
       "std        14.797519        1081.646917      18835.605582      17550.652579   \n",
       "min        -1.000000           0.000000          0.000000          0.000000   \n",
       "25%        32.000000         156.000000      15864.000000       9226.000000   \n",
       "50%        44.000000         685.000000      27178.000000      21705.000000   \n",
       "75%        54.000000        1357.000000      45476.000000      35690.000000   \n",
       "max        73.000000        5687.000000      62262.000000      54389.000000   \n",
       "\n",
       "         cnt_prev_1h  cnt_mean_prev_3h_pickups  cnt_mean_prev_3h_dropoffs  \\\n",
       "count  982163.000000             982163.000000              982163.000000   \n",
       "mean      270.125312                  8.146983                   6.537344   \n",
       "std        81.562234                  6.869240                   6.392805   \n",
       "min         0.000000                  0.000000                   0.000000   \n",
       "25%       241.000000                  2.666667                   1.333333   \n",
       "50%       280.000000                  6.333333                   4.666667   \n",
       "75%       322.000000                 12.333333                  10.000000   \n",
       "max       462.000000                 41.000000                  40.333333   \n",
       "\n",
       "       trip_duration  \n",
       "count  982163.000000  \n",
       "mean      945.804442  \n",
       "std       641.938199  \n",
       "min       301.000000  \n",
       "25%       509.000000  \n",
       "50%       760.000000  \n",
       "75%      1167.000000  \n",
       "max      7331.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9f89404-2abf-4e2d-90d2-11296390dd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'vendor_id', 'pickup_datetime', 'passenger_count',\n",
       "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
       "       'dropoff_latitude', 'store_and_fwd_flag', 'trip_duration',\n",
       "       'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1',\n",
       "       'euclidean_distance', 'pickup_hour_of_day', 'day_of_week',\n",
       "       'hour_of_week', 'month_of_year', 'day_of_year', 'week_of_year',\n",
       "       'hour_of_year', 'pickup_date', 'date', 'r_depth', 's_fall', 's_depth',\n",
       "       'all_precip', 'has_snow', 'has_rain', 'max_temp', 'min_temp',\n",
       "       'pickup_lat_bin', 'pickup_long_bin', 'dropoff_lat_bin',\n",
       "       'dropoff_long_bin', 'cnt_coords_bin_dp', 'cnt_coords_bin_p',\n",
       "       'cnt_coords_bin_d', 'cnt_prev_1h', 'cnt_mean_prev_3h_pickups',\n",
       "       'cnt_mean_prev_3h_dropoffs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25a4601b-59ba-469e-a897-8c7aeac2a60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 303 ms, sys: 221 ms, total: 524 ms\n",
      "Wall time: 523 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'vendor_id', 'passenger_count', 'store_and_fwd_flag',\n",
       "       'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1',\n",
       "       'euclidean_distance', 'pickup_hour_of_day', 'day_of_week',\n",
       "       'hour_of_week', 'month_of_year', 'day_of_year', 'week_of_year',\n",
       "       'hour_of_year', 'r_depth', 's_fall', 's_depth', 'all_precip',\n",
       "       'has_snow', 'has_rain', 'max_temp', 'min_temp', 'cnt_coords_bin_dp',\n",
       "       'cnt_coords_bin_p', 'cnt_coords_bin_d', 'cnt_prev_1h',\n",
       "       'cnt_mean_prev_3h_pickups', 'cnt_mean_prev_3h_dropoffs',\n",
       "       'trip_duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Delete redundant, intermediate columns\n",
    "df_test.drop(columns=[\n",
    "    \"pickup_datetime\", \"pickup_date\", \"date\",\n",
    "    \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "    \"pickup_lat_bin\", \"pickup_long_bin\", \"dropoff_lat_bin\", \"dropoff_long_bin\"\n",
    "], inplace=True)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Reorganize the columns to make `trip_duration` the target column in the end\n",
    "df_test = df_test[[col for col in df_test.columns if col != \"trip_duration\"] + [\"trip_duration\"]]\n",
    "\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05eead04-31b9-4c7c-88c8-5e70bb8ae0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 291713 entries, 0 to 291712\n",
      "Data columns (total 31 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   id                         291713 non-null  object \n",
      " 1   vendor_id                  291713 non-null  int64  \n",
      " 2   passenger_count            291713 non-null  int64  \n",
      " 3   store_and_fwd_flag         291713 non-null  object \n",
      " 4   pickup_pca0                291713 non-null  float64\n",
      " 5   pickup_pca1                291713 non-null  float64\n",
      " 6   dropoff_pca0               291713 non-null  float64\n",
      " 7   dropoff_pca1               291713 non-null  float64\n",
      " 8   euclidean_distance         291713 non-null  float64\n",
      " 9   pickup_hour_of_day         291713 non-null  int32  \n",
      " 10  day_of_week                291713 non-null  int64  \n",
      " 11  hour_of_week               291713 non-null  int64  \n",
      " 12  month_of_year              291713 non-null  int64  \n",
      " 13  day_of_year                291713 non-null  int64  \n",
      " 14  week_of_year               291713 non-null  int64  \n",
      " 15  hour_of_year               291713 non-null  int64  \n",
      " 16  r_depth                    291713 non-null  float64\n",
      " 17  s_fall                     291713 non-null  float64\n",
      " 18  s_depth                    291713 non-null  float64\n",
      " 19  all_precip                 291713 non-null  float64\n",
      " 20  has_snow                   291713 non-null  bool   \n",
      " 21  has_rain                   291713 non-null  bool   \n",
      " 22  max_temp                   291713 non-null  int64  \n",
      " 23  min_temp                   291713 non-null  int64  \n",
      " 24  cnt_coords_bin_dp          291713 non-null  int64  \n",
      " 25  cnt_coords_bin_p           291713 non-null  int64  \n",
      " 26  cnt_coords_bin_d           291713 non-null  int64  \n",
      " 27  cnt_prev_1h                291713 non-null  int64  \n",
      " 28  cnt_mean_prev_3h_pickups   291713 non-null  float64\n",
      " 29  cnt_mean_prev_3h_dropoffs  291713 non-null  float64\n",
      " 30  trip_duration              291713 non-null  int64  \n",
      "dtypes: bool(2), float64(11), int32(1), int64(15), object(2)\n",
      "memory usage: 64.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05f14824-22b8-4dc1-a23b-54f4e5a5112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_pca0</th>\n",
       "      <th>pickup_pca1</th>\n",
       "      <th>dropoff_pca0</th>\n",
       "      <th>dropoff_pca1</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>pickup_hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>all_precip</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>cnt_coords_bin_dp</th>\n",
       "      <th>cnt_coords_bin_p</th>\n",
       "      <th>cnt_coords_bin_d</th>\n",
       "      <th>cnt_prev_1h</th>\n",
       "      <th>cnt_mean_prev_3h_pickups</th>\n",
       "      <th>cnt_mean_prev_3h_dropoffs</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "      <td>291713.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.534351</td>\n",
       "      <td>1.667187</td>\n",
       "      <td>-0.001360</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>-0.003101</td>\n",
       "      <td>0.015176</td>\n",
       "      <td>3.439431</td>\n",
       "      <td>13.617384</td>\n",
       "      <td>3.053823</td>\n",
       "      <td>86.909147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148745</td>\n",
       "      <td>59.401220</td>\n",
       "      <td>43.615135</td>\n",
       "      <td>315.223463</td>\n",
       "      <td>8805.198424</td>\n",
       "      <td>7147.937127</td>\n",
       "      <td>346.387460</td>\n",
       "      <td>10.321529</td>\n",
       "      <td>8.341479</td>\n",
       "      <td>960.215410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498819</td>\n",
       "      <td>1.316759</td>\n",
       "      <td>0.502199</td>\n",
       "      <td>0.823313</td>\n",
       "      <td>0.481242</td>\n",
       "      <td>0.930334</td>\n",
       "      <td>3.956340</td>\n",
       "      <td>6.389152</td>\n",
       "      <td>1.953361</td>\n",
       "      <td>46.795565</td>\n",
       "      <td>...</td>\n",
       "      <td>1.042237</td>\n",
       "      <td>16.947452</td>\n",
       "      <td>14.836900</td>\n",
       "      <td>354.867913</td>\n",
       "      <td>5423.045435</td>\n",
       "      <td>4993.007779</td>\n",
       "      <td>107.054067</td>\n",
       "      <td>8.663103</td>\n",
       "      <td>7.952818</td>\n",
       "      <td>3254.264232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.752336</td>\n",
       "      <td>-7.094127</td>\n",
       "      <td>-14.335127</td>\n",
       "      <td>-7.094127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.095278</td>\n",
       "      <td>-0.374895</td>\n",
       "      <td>-0.148776</td>\n",
       "      <td>-0.421688</td>\n",
       "      <td>1.233456</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4518.000000</td>\n",
       "      <td>3001.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101570</td>\n",
       "      <td>0.088641</td>\n",
       "      <td>0.079633</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>2.094199</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>8456.000000</td>\n",
       "      <td>6835.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>663.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.246469</td>\n",
       "      <td>0.462763</td>\n",
       "      <td>0.238757</td>\n",
       "      <td>0.509920</td>\n",
       "      <td>3.872186</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>13009.000000</td>\n",
       "      <td>10488.000000</td>\n",
       "      <td>415.000000</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>1077.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.016428</td>\n",
       "      <td>27.156156</td>\n",
       "      <td>9.766639</td>\n",
       "      <td>27.055565</td>\n",
       "      <td>115.120552</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>29.610000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1642.000000</td>\n",
       "      <td>18041.000000</td>\n",
       "      <td>15425.000000</td>\n",
       "      <td>572.000000</td>\n",
       "      <td>52.333333</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>86391.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           vendor_id  passenger_count    pickup_pca0    pickup_pca1  \\\n",
       "count  291713.000000    291713.000000  291713.000000  291713.000000   \n",
       "mean        1.534351         1.667187      -0.001360      -0.010228   \n",
       "std         0.498819         1.316759       0.502199       0.823313   \n",
       "min         1.000000         0.000000      -9.752336      -7.094127   \n",
       "25%         1.000000         1.000000      -0.095278      -0.374895   \n",
       "50%         2.000000         1.000000       0.101570       0.088641   \n",
       "75%         2.000000         2.000000       0.246469       0.462763   \n",
       "max         2.000000         7.000000       6.016428      27.156156   \n",
       "\n",
       "        dropoff_pca0   dropoff_pca1  euclidean_distance  pickup_hour_of_day  \\\n",
       "count  291713.000000  291713.000000       291713.000000       291713.000000   \n",
       "mean       -0.003101       0.015176            3.439431           13.617384   \n",
       "std         0.481242       0.930334            3.956340            6.389152   \n",
       "min       -14.335127      -7.094127            0.000000            0.000000   \n",
       "25%        -0.148776      -0.421688            1.233456            9.000000   \n",
       "50%         0.079633       0.096800            2.094199           14.000000   \n",
       "75%         0.238757       0.509920            3.872186           19.000000   \n",
       "max         9.766639      27.055565          115.120552           23.000000   \n",
       "\n",
       "         day_of_week   hour_of_week  ...     all_precip       max_temp  \\\n",
       "count  291713.000000  291713.000000  ...  291713.000000  291713.000000   \n",
       "mean        3.053823      86.909147  ...       0.148745      59.401220   \n",
       "std         1.953361      46.795565  ...       1.042237      16.947452   \n",
       "min         0.000000       0.000000  ...       0.000000      15.000000   \n",
       "25%         1.000000      45.000000  ...       0.000000      46.000000   \n",
       "50%         3.000000      89.000000  ...       0.000000      59.000000   \n",
       "75%         5.000000     128.000000  ...       0.040000      73.000000   \n",
       "max         6.000000     167.000000  ...      29.610000      92.000000   \n",
       "\n",
       "            min_temp  cnt_coords_bin_dp  cnt_coords_bin_p  cnt_coords_bin_d  \\\n",
       "count  291713.000000      291713.000000     291713.000000     291713.000000   \n",
       "mean       43.615135         315.223463       8805.198424       7147.937127   \n",
       "std        14.836900         354.867913       5423.045435       4993.007779   \n",
       "min        -1.000000           0.000000          0.000000          0.000000   \n",
       "25%        32.000000           0.000000       4518.000000       3001.000000   \n",
       "50%        44.000000         216.000000       8456.000000       6835.000000   \n",
       "75%        54.000000         479.000000      13009.000000      10488.000000   \n",
       "max        73.000000        1642.000000      18041.000000      15425.000000   \n",
       "\n",
       "         cnt_prev_1h  cnt_mean_prev_3h_pickups  cnt_mean_prev_3h_dropoffs  \\\n",
       "count  291713.000000             291713.000000              291713.000000   \n",
       "mean      346.387460                 10.321529                   8.341479   \n",
       "std       107.054067                  8.663103                   7.952818   \n",
       "min         0.000000                  0.000000                   0.000000   \n",
       "25%       310.000000                  3.333333                   1.666667   \n",
       "50%       361.000000                  8.000000                   6.333333   \n",
       "75%       415.000000                 15.666667                  12.666667   \n",
       "max       572.000000                 52.333333                  52.000000   \n",
       "\n",
       "       trip_duration  \n",
       "count  291713.000000  \n",
       "mean      960.215410  \n",
       "std      3254.264232  \n",
       "min         1.000000  \n",
       "25%       396.000000  \n",
       "50%       663.000000  \n",
       "75%      1077.000000  \n",
       "max     86391.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87aa5723-a627-4663-a625-495fcd65376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 15.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def recommend_scaling_methods(df):\n",
    "    \"\"\"\n",
    "    Recommend normalization methods based on skewness and kurtosis of the data.\n",
    "    Returns a dictionary where keys are column names and values are the recommended normalization methods.\n",
    "    \"\"\"\n",
    "    scaling_recommendations = {}\n",
    "    \n",
    "    for column in df.select_dtypes(include=[\"int64\", \"float64\", \"int32\", \"float32\"]).columns:\n",
    "        # Skip 'vendor_id' column as it is binary & categorical\n",
    "        if column == \"vendor_id\":\n",
    "            continue\n",
    "        \n",
    "        # Check if the column contains mixed non-numeric values\n",
    "        try:\n",
    "            pd.to_numeric(df[column], errors=\"raise\")\n",
    "            is_mixed = False  # No mixed non-numeric values\n",
    "        except ValueError:\n",
    "            is_mixed = True  # Contains non-numeric values\n",
    "\n",
    "        if is_mixed:\n",
    "            print(f\"Column '{column}' contains non-numeric data and cannot be normalized.\")\n",
    "            scaling_recommendations[column] = None\n",
    "            continue\n",
    "        \n",
    "        # Compute skewness and kurtosis\n",
    "        col_skewness = skew(df[column].dropna())  # Calculate skewness\n",
    "        col_kurtosis = kurtosis(df[column].dropna())  # Calculate kurtosis\n",
    "        \n",
    "        # Count unique values and display up to 10 unique values\n",
    "        unique_values = df[column].dropna().unique()\n",
    "        unique_count = len(unique_values)\n",
    "        print(f\"Column '{column}' has {unique_count} unique values, first 10: {unique_values[:10]}\")\n",
    "        \n",
    "        # Determine the appropriate normalization method based on skewness and kurtosis\n",
    "        if abs(col_skewness) < 0.5 and abs(col_kurtosis) < 3:\n",
    "            scaling_recommendations[column] = \"StandardScaler\"\n",
    "            print(f\"Column '{column}' is approximately normally distributed. Recommended: StandardScaler.\")\n",
    "        else:\n",
    "            scaling_recommendations[column] = \"MinMaxScaler\"\n",
    "            print(f\"Column '{column}' is not normally distributed. Recommended: MinMaxScaler.\")\n",
    "    \n",
    "    return scaling_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39a263f3-8016-43ee-bb53-9160299e50e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'passenger_count' has 7 unique values, first 10: [1 4 3 2 5 6 0]\n",
      "Column 'passenger_count' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'pickup_pca0' has 941889 unique values, first 10: [-1.46272668 -0.22378877 -0.07564223  0.11820193  0.05723002  0.20927621\n",
      "  0.31676871  0.13715185 -0.30037711 -0.13259448]\n",
      "Column 'pickup_pca0' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'pickup_pca1' has 941889 unique values, first 10: [ 0.34018641  0.89645261  0.97908039 -0.41451582  0.04281858 -0.12385815\n",
      " -0.51255053  0.66609366  0.72419467  1.59961465]\n",
      "Column 'pickup_pca1' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'dropoff_pca0' has 962683 unique values, first 10: [-1.06862767  0.02020165  0.19286223  0.34265458  0.04060286  0.45233916\n",
      "  0.22020595 -0.06132744  0.25546429 -0.25607666]\n",
      "Column 'dropoff_pca0' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'dropoff_pca1' has 962683 unique values, first 10: [-0.25515351  0.27692335  0.61128504 -0.68142258 -0.52183294 -0.29450427\n",
      " -0.10244877  1.20386384 -0.45859269  0.87958343]\n",
      "Column 'dropoff_pca1' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'euclidean_distance' has 982160 unique values, first 10: [3.46586978 2.89047725 2.26441684 1.79688963 2.17060041 1.71487811\n",
      " 1.7113663  2.46133732 5.86701551 2.85851752]\n",
      "Column 'euclidean_distance' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'pickup_hour_of_day' has 24 unique values, first 10: [18 13 12 22 16 14 11 21 23  5]\n",
      "Column 'pickup_hour_of_day' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'day_of_week' has 7 unique values, first 10: [2 0 5 6 1 4 3]\n",
      "Column 'day_of_week' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'hour_of_week' has 168 unique values, first 10: [ 66  13 132 138 166 157 136  62  11 141]\n",
      "Column 'hour_of_week' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'month_of_year' has 6 unique values, first 10: [6 4 5 3 2 1]\n",
      "Column 'month_of_year' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'day_of_year' has 182 unique values, first 10: [181 116 128 135 101 129  93 167  81  51]\n",
      "Column 'day_of_year' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'week_of_year' has 27 unique values, first 10: [26 17 18 19 14 13 24 12  7 10]\n",
      "Column 'week_of_year' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'hour_of_year' has 4358 unique values, first 10: [4362 2797 3084 3258 2446 3109 2248 4022 1955 1245]\n",
      "Column 'hour_of_year' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'r_depth' has 35 unique values, first 10: [0.01 0.   0.16 0.06 0.44 0.91 1.65 0.07 2.31 0.02]\n",
      "Column 'r_depth' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 's_fall' has 9 unique values, first 10: [0.00e+00 5.00e-01 1.40e+00 1.00e-02 2.73e+01 2.50e+00 4.00e-01 2.00e-01\n",
      " 1.00e-01]\n",
      "Column 's_fall' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 's_depth' has 10 unique values, first 10: [0.0e+00 1.0e-02 4.0e+00 6.0e+00 1.0e+00 2.0e+00 2.2e+01 9.0e+00 1.9e+01\n",
      " 1.7e+01]\n",
      "Column 's_depth' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'all_precip' has 39 unique values, first 10: [1.000e-02 0.000e+00 1.600e-01 5.600e-01 1.840e+00 9.100e-01 1.650e+00\n",
      " 8.000e-02 2.961e+01 2.000e-02]\n",
      "Column 'all_precip' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'max_temp' has 61 unique values, first 10: [83 69 60 73 50 66 61 85 53 62]\n",
      "Column 'max_temp' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'min_temp' has 59 unique values, first 10: [67 50 48 56 31 49 62 32 39 40]\n",
      "Column 'min_temp' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'cnt_coords_bin_dp' has 736 unique values, first 10: [   0 1937  368 1096  923 1038 1793  401  270  123]\n",
      "Column 'cnt_coords_bin_dp' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'cnt_coords_bin_p' has 124 unique values, first 10: [11920 24463 15281 27178 49700 62262 30418 11877 22948   639]\n",
      "Column 'cnt_coords_bin_p' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'cnt_coords_bin_d' has 218 unique values, first 10: [  937 54389 12775 24316 15035 13338 52181 14986 26951 21850]\n",
      "Column 'cnt_coords_bin_d' is approximately normally distributed. Recommended: StandardScaler.\n",
      "Column 'cnt_prev_1h' has 423 unique values, first 10: [245 266 285 295 224 290 298 304 219 325]\n",
      "Column 'cnt_prev_1h' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'cnt_mean_prev_3h_pickups' has 119 unique values, first 10: [ 5.33333333  8.          4.          7.66666667 16.33333333 15.33333333\n",
      "  3.33333333  8.33333333  0.33333333 17.66666667]\n",
      "Column 'cnt_mean_prev_3h_pickups' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'cnt_mean_prev_3h_dropoffs' has 120 unique values, first 10: [ 0.         18.66666667  3.          7.          5.66666667  3.66666667\n",
      " 13.66666667  4.          4.66666667  5.33333333]\n",
      "Column 'cnt_mean_prev_3h_dropoffs' is not normally distributed. Recommended: MinMaxScaler.\n",
      "Column 'trip_duration' has 5449 unique values, first 10: [1133  887  686  818  951  560 1496  726 1083  797]\n",
      "Column 'trip_duration' is not normally distributed. Recommended: MinMaxScaler.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'passenger_count': 'MinMaxScaler',\n",
       " 'pickup_pca0': 'MinMaxScaler',\n",
       " 'pickup_pca1': 'MinMaxScaler',\n",
       " 'dropoff_pca0': 'MinMaxScaler',\n",
       " 'dropoff_pca1': 'MinMaxScaler',\n",
       " 'euclidean_distance': 'MinMaxScaler',\n",
       " 'pickup_hour_of_day': 'StandardScaler',\n",
       " 'day_of_week': 'StandardScaler',\n",
       " 'hour_of_week': 'StandardScaler',\n",
       " 'month_of_year': 'StandardScaler',\n",
       " 'day_of_year': 'StandardScaler',\n",
       " 'week_of_year': 'MinMaxScaler',\n",
       " 'hour_of_year': 'StandardScaler',\n",
       " 'r_depth': 'MinMaxScaler',\n",
       " 's_fall': 'MinMaxScaler',\n",
       " 's_depth': 'MinMaxScaler',\n",
       " 'all_precip': 'MinMaxScaler',\n",
       " 'max_temp': 'StandardScaler',\n",
       " 'min_temp': 'StandardScaler',\n",
       " 'cnt_coords_bin_dp': 'MinMaxScaler',\n",
       " 'cnt_coords_bin_p': 'StandardScaler',\n",
       " 'cnt_coords_bin_d': 'StandardScaler',\n",
       " 'cnt_prev_1h': 'MinMaxScaler',\n",
       " 'cnt_mean_prev_3h_pickups': 'MinMaxScaler',\n",
       " 'cnt_mean_prev_3h_dropoffs': 'MinMaxScaler',\n",
       " 'trip_duration': 'MinMaxScaler'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling_recommendations = recommend_scaling_methods(df_train)\n",
    "scaling_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b508b61-1573-4d18-b45d-88e54695ba2e",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0a8e396-b9a8-4879-bfe9-761239f1da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 302 ms, sys: 30.4 ms, total: 332 ms\n",
      "Wall time: 331 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "bool_columns = [\"has_snow\", \"has_rain\"]\n",
    "df_train[bool_columns] = df_train[bool_columns].astype(int)\n",
    "df_test[bool_columns] = df_test[bool_columns].astype(int)\n",
    "    \n",
    "# Process the vendor_id column\n",
    "if \"vendor_id\" in df_train.columns:\n",
    "    df_train[\"vendor_id\"] = df_train[\"vendor_id\"] - 1\n",
    "    \n",
    "if \"vendor_id\" in df_test.columns:\n",
    "    df_test[\"vendor_id\"] = df_test[\"vendor_id\"] - 1\n",
    "    \n",
    "# Progress the flag column\n",
    "df_train[\"store_and_fwd_flag\"] = df_train[\"store_and_fwd_flag\"].apply(lambda x: 0 if x == \"Y\" else 1)\n",
    "df_test[\"store_and_fwd_flag\"] = df_test[\"store_and_fwd_flag\"].apply(lambda x: 0 if x == \"Y\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0786d-d550-4757-afb7-322f901265a0",
   "metadata": {},
   "source": [
    "### Standardization & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "295844c9-2a98-4a89-8b43-3f0de30aad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 440 ms, sys: 54.5 ms, total: 495 ms\n",
      "Wall time: 492 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def scale_data(df_train: pd.DataFrame, df_test: pd.DataFrame, scaling_dict: dict):\n",
    "    \"\"\"\n",
    "    Scales the training and testing data based on the provided scaling dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    df_train (pd.DataFrame): Training dataset\n",
    "    df_test (pd.DataFrame): Testing dataset\n",
    "    scaling_dict (dict): Dictionary specifying the scaler type for each column\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Transformed df_train, df_test, and the scaler used for 'trip_duration' if present\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variable to store scaler for 'trip_duration'\n",
    "    trip_duration_scaler = None\n",
    "    \n",
    "    for col, scaler_name in scaling_dict.items():\n",
    "        if scaler_name == \"MinMaxScaler\":\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_name == \"StandardScaler\":\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scaler: {scaler_name}\")\n",
    "            \n",
    "        # Apply scaling if column exists in both train and test datasets\n",
    "        if col in df_train.columns and col in df_test.columns:\n",
    "            # Fit and transform training data\n",
    "            df_train[col] = scaler.fit_transform(df_train[[col]])\n",
    "            \n",
    "            # Transform test data using the same scaler\n",
    "            df_test[col] = scaler.transform(df_test[[col]])\n",
    "        \n",
    "            # Store the scaler used for 'trip_duration' if it exists\n",
    "            if col == \"trip_duration\":\n",
    "                trip_duration_scaler = scaler\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported column: {col}\")\n",
    "    \n",
    "    return df_train, df_test, trip_duration_scaler\n",
    "\n",
    "df_train, df_test, trip_duration_scaler = scale_data(\n",
    "    df_train, df_test, scaling_recommendations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c671a294-6a2c-4022-8cb8-bdb11139261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 982163 entries, 0 to 982162\n",
      "Data columns (total 31 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   id                         982163 non-null  object \n",
      " 1   vendor_id                  982163 non-null  int64  \n",
      " 2   passenger_count            982163 non-null  float64\n",
      " 3   store_and_fwd_flag         982163 non-null  int64  \n",
      " 4   pickup_pca0                982163 non-null  float64\n",
      " 5   pickup_pca1                982163 non-null  float64\n",
      " 6   dropoff_pca0               982163 non-null  float64\n",
      " 7   dropoff_pca1               982163 non-null  float64\n",
      " 8   euclidean_distance         982163 non-null  float64\n",
      " 9   pickup_hour_of_day         982163 non-null  float64\n",
      " 10  day_of_week                982163 non-null  float64\n",
      " 11  hour_of_week               982163 non-null  float64\n",
      " 12  month_of_year              982163 non-null  float64\n",
      " 13  day_of_year                982163 non-null  float64\n",
      " 14  week_of_year               982163 non-null  float64\n",
      " 15  hour_of_year               982163 non-null  float64\n",
      " 16  r_depth                    982163 non-null  float64\n",
      " 17  s_fall                     982163 non-null  float64\n",
      " 18  s_depth                    982163 non-null  float64\n",
      " 19  all_precip                 982163 non-null  float64\n",
      " 20  has_snow                   982163 non-null  int64  \n",
      " 21  has_rain                   982163 non-null  int64  \n",
      " 22  max_temp                   982163 non-null  float64\n",
      " 23  min_temp                   982163 non-null  float64\n",
      " 24  cnt_coords_bin_dp          982163 non-null  float64\n",
      " 25  cnt_coords_bin_p           982163 non-null  float64\n",
      " 26  cnt_coords_bin_d           982163 non-null  float64\n",
      " 27  cnt_prev_1h                982163 non-null  float64\n",
      " 28  cnt_mean_prev_3h_pickups   982163 non-null  float64\n",
      " 29  cnt_mean_prev_3h_dropoffs  982163 non-null  float64\n",
      " 30  trip_duration              982163 non-null  float64\n",
      "dtypes: float64(26), int64(4), object(1)\n",
      "memory usage: 232.3+ MB\n",
      "CPU times: user 104 ms, sys: 60.7 ms, total: 164 ms\n",
      "Wall time: 161 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b208641d-4fe8-41ad-801a-35c399614afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 291713 entries, 0 to 291712\n",
      "Data columns (total 31 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   id                         291713 non-null  object \n",
      " 1   vendor_id                  291713 non-null  int64  \n",
      " 2   passenger_count            291713 non-null  float64\n",
      " 3   store_and_fwd_flag         291713 non-null  int64  \n",
      " 4   pickup_pca0                291713 non-null  float64\n",
      " 5   pickup_pca1                291713 non-null  float64\n",
      " 6   dropoff_pca0               291713 non-null  float64\n",
      " 7   dropoff_pca1               291713 non-null  float64\n",
      " 8   euclidean_distance         291713 non-null  float64\n",
      " 9   pickup_hour_of_day         291713 non-null  float64\n",
      " 10  day_of_week                291713 non-null  float64\n",
      " 11  hour_of_week               291713 non-null  float64\n",
      " 12  month_of_year              291713 non-null  float64\n",
      " 13  day_of_year                291713 non-null  float64\n",
      " 14  week_of_year               291713 non-null  float64\n",
      " 15  hour_of_year               291713 non-null  float64\n",
      " 16  r_depth                    291713 non-null  float64\n",
      " 17  s_fall                     291713 non-null  float64\n",
      " 18  s_depth                    291713 non-null  float64\n",
      " 19  all_precip                 291713 non-null  float64\n",
      " 20  has_snow                   291713 non-null  int64  \n",
      " 21  has_rain                   291713 non-null  int64  \n",
      " 22  max_temp                   291713 non-null  float64\n",
      " 23  min_temp                   291713 non-null  float64\n",
      " 24  cnt_coords_bin_dp          291713 non-null  float64\n",
      " 25  cnt_coords_bin_p           291713 non-null  float64\n",
      " 26  cnt_coords_bin_d           291713 non-null  float64\n",
      " 27  cnt_prev_1h                291713 non-null  float64\n",
      " 28  cnt_mean_prev_3h_pickups   291713 non-null  float64\n",
      " 29  cnt_mean_prev_3h_dropoffs  291713 non-null  float64\n",
      " 30  trip_duration              291713 non-null  float64\n",
      "dtypes: float64(26), int64(4), object(1)\n",
      "memory usage: 69.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f507a63-4abe-4eb0-99d7-cb478c3dffa1",
   "metadata": {},
   "source": [
    "### Save Data in parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "937ff5b0-54a4-4492-8b0b-de0b3751b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train and df_test saved to 'prep' directory as Parquet files.\n",
      "CPU times: user 1.11 s, sys: 265 ms, total: 1.37 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Ensure the 'prep' directory exists\n",
    "os.makedirs(os.path.join(\"data\", \"prep\"), exist_ok=True)\n",
    "\n",
    "# Save to Parquet format\n",
    "df_train.to_parquet(\"data/prep/df_train.parquet\", index=False)\n",
    "df_test.to_parquet(\"data/prep/df_test.parquet\", index=False)\n",
    "\n",
    "print(\"df_train and df_test saved to 'prep' directory as Parquet files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea8ccf-0549-4236-a9fc-937cf4a11fe1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
