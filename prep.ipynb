{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab2b8df-2342-462a-9c7b-f39c2821a259",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Trip Duration\n",
    "\n",
    "Generate code & functions such that conducts data preprocessing(includes feature engineering & data cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb9d45-f563-46f0-b387-cd90597809b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import Data Handling Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import Date-Time Handling Libraries\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "\n",
    "# Import Geodetic Libraries\n",
    "import pyproj\n",
    "from pyproj import Geod\n",
    "\n",
    "# Import Data Visualization Libraries\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.size\"] = 12\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 12]  # Set default figure size\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Machine Learning Libraries\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis\n",
    "\n",
    "# Set random seed for reproducibility in scikit-learn\n",
    "from sklearn.utils import check_random_state\n",
    "rng = check_random_state(42)\n",
    "\n",
    "# Import Utilities\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# Import Custom Modules\n",
    "from data_loader import *  # Custom data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15788607-720c-45b5-854c-8e4f409eb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the dataset\n",
    "df_train = load_data(\"train\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107f50a-eb4c-4dd8-8488-71e99a0650ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the test dataset\n",
    "df_test = load_data(\"test\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b1bff-dd6d-4481-9a9b-473b2815cbb0",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71558b23-63ca-4c2e-88f7-7f91270f0ce5",
   "metadata": {},
   "source": [
    "### PCA in Longitudes & Latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e8fa9-b231-47ee-b54b-7557f0d3b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def apply_pca_to_coords(train, test, random_seed=42):\n",
    "    \"\"\"\n",
    "    Applies PCA transformation to pickup and dropoff coordinates for train and test datasets.\n",
    "\n",
    "    The PCA is fitted **only on the training data** to prevent data leakage.\n",
    "\n",
    "    Parameters:\n",
    "        train (pd.DataFrame): The training dataset.\n",
    "        test (pd.DataFrame): The testing dataset.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies train and test DataFrames in place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit PCA only on training data\n",
    "    coords_train = np.vstack((\n",
    "        train[[\"pickup_latitude\", \"pickup_longitude\"]].values,\n",
    "        train[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    ))\n",
    "\n",
    "    pca = PCA(whiten=True, random_seed=random_seed).fit(coords_train)\n",
    "\n",
    "    # Apply transformation to train dataset\n",
    "    train.loc[:, \"pickup_pca0\"] = pca.transform(train[[\"pickup_latitude\", \"pickup_longitude\"]])[:, 0]\n",
    "    train.loc[:, \"pickup_pca1\"] = pca.transform(train[[\"pickup_latitude\", \"pickup_longitude\"]])[:, 1]\n",
    "    train.loc[:, \"dropoff_pca0\"] = pca.transform(train[[\"dropoff_latitude\", \"dropoff_longitude\"]])[:, 0]\n",
    "    train.loc[:, \"dropoff_pca1\"] = pca.transform(train[[\"dropoff_latitude\", \"dropoff_longitude\"]])[:, 1]\n",
    "\n",
    "    # Apply the same transformation to test dataset to avoid data leakage\n",
    "    test.loc[:, \"pickup_pca0\"] = pca.transform(test[[\"pickup_latitude\", \"pickup_longitude\"]])[:, 0]\n",
    "    test.loc[:, \"pickup_pca1\"] = pca.transform(test[[\"pickup_latitude\", \"pickup_longitude\"]])[:, 1]\n",
    "    test.loc[:, \"dropoff_pca0\"] = pca.transform(test[[\"dropoff_latitude\", \"dropoff_longitude\"]])[:, 0]\n",
    "    test.loc[:, \"dropoff_pca1\"] = pca.transform(test[[\"dropoff_latitude\", \"dropoff_longitude\"]])[:, 1]\n",
    "\n",
    "# Example usage:\n",
    "apply_pca_to_coords(df_train, df_test, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814f521-b108-4543-91a0-5095f831fb1c",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a9992-3bcc-422a-9c7d-8318cbaa6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define WGS84 ellipsoid\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Compute great-circle distance in kilometers\n",
    "df_train[\"euclidean_distance\"] = df_train.apply(\n",
    "    lambda row: geod.inv(row[\"pickup_longitude\"], row[\"pickup_latitude\"],\n",
    "                         row[\"dropoff_longitude\"], row[\"dropoff_latitude\"])[2] / 1000, axis=1\n",
    ")\n",
    "\n",
    "# Compute great-circle distance in kilometers\n",
    "df_test[\"euclidean_distance\"] = df_test.apply(\n",
    "    lambda row: geod.inv(row[\"pickup_longitude\"], row[\"pickup_latitude\"],\n",
    "                         row[\"dropoff_longitude\"], row[\"dropoff_latitude\"])[2] / 1000, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c2049-3816-4d41-9718-fdd59f412c65",
   "metadata": {},
   "source": [
    "### Datetime Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c665f53-2be9-4270-b4a3-bada27f573e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def generate_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Generate detailed date-time features for pickups and modify the DataFrame in place.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the datetime column.\n",
    "    \n",
    "    Returns:\n",
    "        None (Modifies df in place)\n",
    "    \"\"\"\n",
    "    # Convert to datetime format\n",
    "    pickup_times = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "\n",
    "    # Extract relevant time features\n",
    "    df[\"pickup_hour_of_day\"] = (pickup_times.dt.hour * 60.0 + pickup_times.dt.minute) / 60.0\n",
    "    df[\"dropoff_hour_of_day\"] = df[\"pickup_hour_of_day\"] + df[\"trip_duration [min]\"] / 60.0\n",
    "\n",
    "    df[\"day_of_week\"] = pickup_times.dt.weekday\n",
    "    df[\"hour_of_week\"] = df[\"day_of_week\"] * 24.0 + df[\"pickup_hour_of_day\"]\n",
    "\n",
    "    df[\"month_of_year\"] = pickup_times.dt.month\n",
    "    df[\"day_of_year\"] = pickup_times.dt.dayofyear\n",
    "    df[\"week_of_year\"] = pickup_times.dt.isocalendar().week\n",
    "    df[\"hour_of_year\"] = df[\"day_of_year\"] * 24.0 + df[\"pickup_hour_of_day\"]\n",
    "\n",
    "generate_datetime_features(df_train)\n",
    "generate_datetime_features(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00651e3-34a4-4264-8012-d7b59958eda4",
   "metadata": {},
   "source": [
    "### Temporal & Geospatial Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729d5e0-34b6-41db-a562-fa3bfa155c86",
   "metadata": {},
   "source": [
    "### NYC Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10577d-68e2-44e6-9a4b-191c14a6e0c0",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c71eaa-60cb-4292-83b1-af4296aeb41f",
   "metadata": {},
   "source": [
    "### Location Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12284ae-e52d-453c-9129-0d4d3aa5c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_nyc_boundary(df, geojson_path):\n",
    "    \"\"\"\n",
    "    Filters pickup and dropoff locations to keep only those within the New York City boundary.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing pickup and dropoff coordinates.\n",
    "        geojson_path (str): Path to the GeoJSON file defining NYC boundaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with locations inside the NYC bounding box.\n",
    "    \"\"\"\n",
    "    # Load the GeoJSON file\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    # Extract NYC boundary coordinates where NAME is \"New York\"\n",
    "    nyc_coords = []\n",
    "    for feature in geojson_data[\"features\"]:\n",
    "        if feature[\"properties\"].get(\"NAME\") == \"New York\":\n",
    "            for polygon in feature[\"geometry\"][\"coordinates\"]:  # Loop through MultiPolygon\n",
    "                for ring in polygon:  # Each polygon has a ring of coordinates\n",
    "                    nyc_coords.extend(ring)\n",
    "\n",
    "    # Compute NYC bounding box (min/max latitudes & longitudes)\n",
    "    min_long = min(lon for lon, lat in nyc_coords)\n",
    "    max_long = max(lon for lon, lat in nyc_coords)\n",
    "    min_lat = min(lat for lon, lat in nyc_coords)\n",
    "    max_lat = max(lat for lon, lat in nyc_coords)\n",
    "\n",
    "    # Filter data based on bounding box\n",
    "    mask = (\n",
    "        (df[\"pickup_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"pickup_latitude\"].between(min_lat, max_lat))\n",
    "        & (df[\"dropoff_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"dropoff_latitude\"].between(min_lat, max_lat))\n",
    "    )\n",
    "\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "# Apply function to df_train and df_test\n",
    "df_train = filter_by_nyc_boundary(df_train, \"utils/gz_2010_us_040_00_5m.json\")\n",
    "df_test = filter_by_nyc_boundary(df_test, \"utils/gz_2010_us_040_00_5m.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d40a5-30bb-4057-bb02-02110a4556ac",
   "metadata": {},
   "source": [
    "### Duration Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e24d07-f4e9-4249-adab-a9c78085dec1",
   "metadata": {},
   "source": [
    "### Speed Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea8ccf-0549-4236-a9fc-937cf4a11fe1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
