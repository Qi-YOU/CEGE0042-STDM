{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab2b8df-2342-462a-9c7b-f39c2821a259",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Trip Duration\n",
    "\n",
    "Generate code & functions such that conducts data preprocessing(includes feature engineering & data cleaning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "defb9d45-f563-46f0-b387-cd90597809b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.49 s, sys: 611 ms, total: 2.1 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import Data Handling Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import Date-Time Handling Libraries\n",
    "from datetime import timedelta\n",
    "import datetime as dt\n",
    "\n",
    "# Import Geodetic Libraries\n",
    "import pyproj\n",
    "from pyproj import Geod\n",
    "\n",
    "# Import Data Visualization Libraries\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.size\"] = 12\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 12]  # Set default figure size\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Machine Learning Libraries\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis\n",
    "\n",
    "# Set random seed for reproducibility in scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "rng = check_random_state(42)\n",
    "\n",
    "# Import Utilities\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import json\n",
    "\n",
    "# Import Custom Modules\n",
    "from data_loader import *  # Custom data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15788607-720c-45b5-854c-8e4f409eb38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.93 s, sys: 1.06 s, total: 3.98 s\n",
      "Wall time: 4.01 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4  id2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"data.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21c7161-4d10-4bb0-b02a-dc060bacce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1458644 entries, 0 to 1458643\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count    Dtype  \n",
      "---  ------              --------------    -----  \n",
      " 0   id                  1458644 non-null  object \n",
      " 1   vendor_id           1458644 non-null  int64  \n",
      " 2   pickup_datetime     1458644 non-null  object \n",
      " 3   dropoff_datetime    1458644 non-null  object \n",
      " 4   passenger_count     1458644 non-null  int64  \n",
      " 5   pickup_longitude    1458644 non-null  float64\n",
      " 6   pickup_latitude     1458644 non-null  float64\n",
      " 7   dropoff_longitude   1458644 non-null  float64\n",
      " 8   dropoff_latitude    1458644 non-null  float64\n",
      " 9   store_and_fwd_flag  1458644 non-null  object \n",
      " 10  trip_duration       1458644 non-null  int64  \n",
      "dtypes: float64(4), int64(3), object(4)\n",
      "memory usage: 122.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34d95c2-ffb2-4507-a23f-bd072c4ba113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.3 ms, sys: 39.8 ms, total: 99.1 ms\n",
      "Wall time: 98.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Delete cols that leads to data leakage\n",
    "del df[\"dropoff_datetime\"]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f979079-f2a5-4b2e-a991-f70b65f5e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define helper function formats time seconds into string\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{hours} hour {minutes} min {seconds:.2f} sec\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b1bff-dd6d-4481-9a9b-473b2815cbb0",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71558b23-63ca-4c2e-88f7-7f91270f0ce5",
   "metadata": {},
   "source": [
    "### PCA in Longitudes & Latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75e8fa9-b231-47ee-b54b-7557f0d3b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 1.02 s, total: 2.12 s\n",
      "Wall time: 477 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def apply_pca_to_coords(df, random_seed=42):\n",
    "    \"\"\"\n",
    "    Applies PCA transformation to pickup and dropoff coordinates for train and test datasets.\n",
    "\n",
    "    The PCA is fitted **only on the training data** to prevent data leakage.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies train and test DataFrames in place.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit PCA on data\n",
    "    coords_train = np.vstack((\n",
    "        df[[\"pickup_latitude\", \"pickup_longitude\"]].values,\n",
    "        df[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    ))\n",
    "\n",
    "    pca = PCA(whiten=True, random_state=random_seed).fit(coords_train)\n",
    "\n",
    "    # Apply transformation to train dataset\n",
    "    df_coords_pickup = df[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "    df_coords_dropoff = df[[\"dropoff_latitude\", \"dropoff_longitude\"]].values\n",
    "    df.loc[:, \"pickup_pca0\"] = pca.transform(df_coords_pickup)[:, 0]\n",
    "    df.loc[:, \"pickup_pca1\"] = pca.transform(df_coords_pickup)[:, 1]\n",
    "    df.loc[:, \"dropoff_pca0\"] = pca.transform(df_coords_dropoff)[:, 0]\n",
    "    df.loc[:, \"dropoff_pca1\"] = pca.transform(df_coords_dropoff)[:, 1]\n",
    "\n",
    "# Example usage:\n",
    "apply_pca_to_coords(df, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814f521-b108-4543-91a0-5095f831fb1c",
   "metadata": {},
   "source": [
    "### Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7a9992-3bcc-422a-9c7d-8318cbaa6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 1.1 s, total: 18.9 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define WGS84 ellipsoid\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Compute great-circle distance in kilometers\n",
    "df[\"geodesic_distance\"] = df.apply(\n",
    "    lambda row: geod.inv(row[\"pickup_longitude\"], row[\"pickup_latitude\"],\n",
    "                         row[\"dropoff_longitude\"], row[\"dropoff_latitude\"])[2] / 1000, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7c2049-3816-4d41-9718-fdd59f412c65",
   "metadata": {},
   "source": [
    "### Datetime Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c665f53-2be9-4270-b4a3-bada27f573e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 59.6 ms, total: 1.18 s\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generate_datetime_features(df):\n",
    "    \"\"\"\n",
    "    Generate detailed date-time features for pickups and modify the DataFrame in place.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the datetime column.\n",
    "    \n",
    "    Returns:\n",
    "        None (Modifies df in place)\n",
    "    \"\"\"\n",
    "    # Convert to datetime format\n",
    "    pickup_times = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "\n",
    "    # Extract relevant time features as integers\n",
    "    df[\"pickup_hour_of_day\"] = (pickup_times.dt.hour * 60 + pickup_times.dt.minute) // 60  # Integer division\n",
    "\n",
    "    df[\"day_of_week\"] = pickup_times.dt.weekday.astype(int)\n",
    "    df[\"hour_of_week\"] = (df[\"day_of_week\"] * 24 + df[\"pickup_hour_of_day\"]).astype(int)\n",
    "\n",
    "    df[\"month_of_year\"] = pickup_times.dt.month.astype(int)\n",
    "    df[\"day_of_year\"] = pickup_times.dt.dayofyear.astype(int)\n",
    "    df[\"week_of_year\"] = pickup_times.dt.isocalendar().week.astype(int)\n",
    "    df[\"hour_of_year\"] = (df[\"day_of_year\"] * 24 + df[\"pickup_hour_of_day\"]).astype(int)\n",
    "\n",
    "generate_datetime_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729d5e0-34b6-41db-a562-fa3bfa155c86",
   "metadata": {},
   "source": [
    "### NYC Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56df2219-78a3-4ddd-bcb6-ae3701e5c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 982 ms, sys: 344 ms, total: 1.33 s\n",
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def merge_weather_data(df):\n",
    "    \"\"\"\n",
    "    Merges weather data with a given dataframe (train or test) based on the pickup date.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The train or test dataframe containing 'pickup_datetime'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The merged dataframe with only the intermediate weather data columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load NYC weather data to enrich information\n",
    "    weather_data = pd.read_csv(os.path.join(\"utils\", \"weather_data_nyc_centralpark_2016.csv\"), low_memory=False)\n",
    "    weather_data[\"date\"] = pd.to_datetime(weather_data[\"date\"], format=\"%d-%m-%Y\")\n",
    "\n",
    "    # Ensure datetime consistency\n",
    "    weather_data[\"date\"] = weather_data[\"date\"].dt.date\n",
    "    df[\"pickup_date\"] = pd.to_datetime(df[\"pickup_datetime\"]).dt.date\n",
    "\n",
    "    # Handle trace values in precipitation, snow fall, and snow depth columns\n",
    "    weather_data[\"r_depth\"] = weather_data[\"precipitation\"].apply(lambda x: 0.01 if x == \"T\" else float(x))  # rain depth\n",
    "    weather_data[\"s_fall\"] = weather_data[\"snow fall\"].apply(lambda x: 0.01 if x == \"T\" else float(x))  # snow fall\n",
    "    weather_data[\"s_depth\"] = weather_data[\"snow depth\"].apply(lambda x: 0.01 if x == \"T\" else float(x))  # snow depth\n",
    "\n",
    "    # Calculate total precipitation, and snow/rain indicators\n",
    "    weather_data[\"all_precip\"] = weather_data[\"s_fall\"] + weather_data[\"r_depth\"]\n",
    "    weather_data[\"has_snow\"] = (weather_data[\"s_fall\"] > 0) | (weather_data[\"s_depth\"] > 0)\n",
    "    weather_data[\"has_rain\"] = weather_data[\"r_depth\"] > 0\n",
    "\n",
    "    # Copy temperature columns\n",
    "    weather_data[\"max_temp\"] = weather_data[\"maximum temperature\"]\n",
    "    weather_data[\"min_temp\"] = weather_data[\"minimum temperature\"]\n",
    "\n",
    "    # Select only the newly created columns\n",
    "    weather_data = weather_data[[\"date\", \"r_depth\", \"s_fall\", \"s_depth\", \"all_precip\", \"has_snow\", \"has_rain\", \"max_temp\", \"min_temp\"]]\n",
    "\n",
    "    # Merge the datasets on the date\n",
    "    df = df.merge(weather_data, left_on=\"pickup_date\", right_on=\"date\", how=\"left\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply function to train and test datasets\n",
    "df = merge_weather_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10577d-68e2-44e6-9a4b-191c14a6e0c0",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c71eaa-60cb-4292-83b1-af4296aeb41f",
   "metadata": {},
   "source": [
    "### Location Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a12284ae-e52d-453c-9129-0d4d3aa5c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records before filtering: 1458644\n",
      "Records after filtering: 1458577\n",
      "Records dropped: 67\n",
      "\n",
      "CPU times: user 366 ms, sys: 90.9 ms, total: 456 ms\n",
      "Wall time: 451 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_nyc_boundary(df, geojson_path):\n",
    "    \"\"\"\n",
    "    Filters pickup and dropoff locations to keep only those within the New York City boundary.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing pickup and dropoff coordinates.\n",
    "        geojson_path (str): Path to the GeoJSON file defining NYC boundaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with locations inside the NYC bounding box.\n",
    "    \"\"\"\n",
    "    # Load the GeoJSON file\n",
    "    with open(geojson_path, \"r\") as f:\n",
    "        geojson_data = json.load(f)\n",
    "\n",
    "    # Extract NYC boundary coordinates where NAME is \"New York\"\n",
    "    nyc_coords = []\n",
    "    for feature in geojson_data[\"features\"]:\n",
    "        if feature[\"properties\"].get(\"NAME\") == \"New York\":\n",
    "            for polygon in feature[\"geometry\"][\"coordinates\"]:  # Loop through MultiPolygon\n",
    "                for ring in polygon:  # Each polygon has a ring of coordinates\n",
    "                    nyc_coords.extend(ring)\n",
    "\n",
    "    # Compute NYC bounding box (min/max latitudes & longitudes)\n",
    "    min_long = min(lon for lon, lat in nyc_coords)\n",
    "    max_long = max(lon for lon, lat in nyc_coords)\n",
    "    min_lat = min(lat for lon, lat in nyc_coords)\n",
    "    max_lat = max(lat for lon, lat in nyc_coords)\n",
    "\n",
    "    # Count records before filtering\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # Filter data based on bounding box\n",
    "    mask = (\n",
    "        (df[\"pickup_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"pickup_latitude\"].between(min_lat, max_lat))\n",
    "        & (df[\"dropoff_longitude\"].between(min_long, max_long))\n",
    "        & (df[\"dropoff_latitude\"].between(min_lat, max_lat))\n",
    "    )\n",
    "\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    # Count records after filtering\n",
    "    final_count = len(filtered_df)\n",
    "    dropped_count = initial_count - final_count\n",
    "\n",
    "    print(f\"Records before filtering: {initial_count}\")\n",
    "    print(f\"Records after filtering: {final_count}\")\n",
    "    print(f\"Records dropped: {dropped_count}\\n\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Apply function to df\n",
    "df = filter_by_nyc_boundary(df, \"utils/gz_2010_us_040_00_5m.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b448cff-b9fa-4483-bbb1-92e28066cfb7",
   "metadata": {},
   "source": [
    "### Distance & Duration Outlier\n",
    "\n",
    "Some outliers represent natural variations in the population, and they should be left as is in your dataset. These are called true outliers. Other outliers are problematic and should be removed because they represent measurement errors, data entry or processing errors, or poor sampling, e.g. zero-distance trip or trips with too fast speed.. This piece of code filters distance and duration outliers in both train & test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df8466ff-49f7-4d6d-a9f8-8ae081563155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Distance: 671.44 km\n",
      "Farthest Points: ((-71.856214, 41.070598), (-79.761951, 42.26986))\n",
      "CPU times: user 2.5 s, sys: 7.95 ms, total: 2.51 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define WGS84 ellipsoid\n",
    "geod = Geod(ellps=\"WGS84\")\n",
    "\n",
    "# Load the GeoJSON file to Calculate the MAX distance possible for a trip in New York\n",
    "geojson_path = \"utils/gz_2010_us_040_00_5m.json\"\n",
    "with open(geojson_path, \"r\") as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Extract NYC boundary coordinates where NAME is \"New York\"\n",
    "nyc_coords = []\n",
    "for feature in geojson_data[\"features\"]:\n",
    "    if feature[\"properties\"].get(\"NAME\") == \"New York\":\n",
    "        for polygon in feature[\"geometry\"][\"coordinates\"]:  # Loop through MultiPolygon\n",
    "            for ring in polygon:  # Each polygon has a ring of coordinates\n",
    "                nyc_coords.extend(ring)\n",
    "\n",
    "# Find the two farthest points in the boundary\n",
    "max_distance = 0\n",
    "max_pair = None\n",
    "\n",
    "for i in range(len(nyc_coords)):\n",
    "    for j in range(i + 1, len(nyc_coords)):\n",
    "        lon1, lat1 = nyc_coords[i]\n",
    "        lon2, lat2 = nyc_coords[j]\n",
    "\n",
    "        # Compute geodesic distance in meters\n",
    "        _, _, dist_m = geod.inv(lon1, lat1, lon2, lat2)\n",
    "\n",
    "        # Convert to kilometers\n",
    "        dist_km = dist_m / 1000\n",
    "\n",
    "        if dist_km > max_distance:\n",
    "            max_distance = dist_km\n",
    "            max_pair = ((lon1, lat1), (lon2, lat2))\n",
    "\n",
    "print(f\"Max Distance: {max_distance:.2f} km\")\n",
    "print(f\"Farthest Points: {max_pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18fa224-e2c7-4ecf-9650-11735c5fff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied lower bound (geodesic_distance): 0.1\n",
      "Applied upper bound (geodesic_distance): 720\n",
      "Total records dropped due to geodesic_distance outliers: 13287\n",
      "\n",
      "Applied lower bound (trip_duration): 300\n",
      "Applied upper bound (trip_duration): 72000\n",
      "Total records dropped due to trip_duration outliers: 215491\n",
      "CPU times: user 934 ms, sys: 788 ms, total: 1.72 s\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_bounds(df, column, lower_bound=None, upper_bound=None):\n",
    "    \"\"\"\n",
    "    Filters trips based on a given column (e.g., Euclidean distance or trip duration)\n",
    "    using absolute lower and upper bounds only.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the column to filter.\n",
    "        column (str): The column to apply filtering on.\n",
    "        lower_bound (float, optional): Absolute minimum value to keep. Defaults to None (not applied).\n",
    "        upper_bound (float, optional): Absolute maximum value to keep. Defaults to None (not applied).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with values within the specified bounds.\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    if lower_bound is not None:\n",
    "        df = df[df[column] > lower_bound]\n",
    "        print(f\"Applied lower bound ({column}): {lower_bound}\")\n",
    "    \n",
    "    if upper_bound is not None:\n",
    "        df = df[df[column] < upper_bound]\n",
    "        print(f\"Applied upper bound ({column}): {upper_bound}\")\n",
    "    \n",
    "    final_count = len(df)\n",
    "    dropped_count = initial_count - final_count\n",
    "    print(f\"Total records dropped due to {column} outliers: {dropped_count}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply function to filter both geodesic_distance and trip_duration\n",
    "# Speed Lim: [1m/s ~ 25m/s] <-> [3.6 km/h ~ 90km/h]\n",
    "# Duration Lim: [5min ~ 20h] <-> [300s ~ 72000]\n",
    "df = filter_by_bounds(df, \"geodesic_distance\", lower_bound=0.1, upper_bound=720)      # [0.1km, 720km]\n",
    "print() \n",
    "df = filter_by_bounds(df, \"trip_duration\", lower_bound=300, upper_bound=72_000)       # [5min, 20h]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d17fff-55e9-474c-8074-10773c19e22b",
   "metadata": {},
   "source": [
    "### Speed Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4605d5a8-91ab-4f2e-9223-1d6a2b43b0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def filter_by_speed(df, distance_col, duration_col, speed_lower_limit, speed_upper_limit):\n",
    "    \"\"\"\n",
    "    Filters trips based on both speed and duration limits, using distance and duration columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the columns to filter.\n",
    "        distance_col (str): The column for geodesic distance.\n",
    "        duration_col (str): The column for trip duration.\n",
    "        speed_lower_limit (float): Minimum speed limit in m/s.\n",
    "        speed_upper_limit (float): Maximum speed limit in m/s.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with values within the specified bounds for speed and duration.\n",
    "    \"\"\"\n",
    "    # Calculate speed from distance and duration\n",
    "    df[\"speed\"] = (df[distance_col] / df[duration_col]) * 1000\n",
    "    \n",
    "    # Apply the filtering\n",
    "    df = filter_by_bounds(df, \"speed\", lower_bound=speed_lower_limit, upper_bound=speed_upper_limit)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2ef07f-b3ed-4473-9e48-43e32aaaa604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied lower bound (speed): 1\n",
      "Applied upper bound (speed): 25\n",
      "Total records dropped due to speed outliers: 18812\n",
      "CPU times: user 338 ms, sys: 52.4 ms, total: 391 ms\n",
      "Wall time: 388 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply the function to filter speed outliers\n",
    "df = filter_by_speed(\n",
    "    df,\n",
    "    \"geodesic_distance\", \"trip_duration\",\n",
    "    speed_lower_limit=1, speed_upper_limit=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00651e3-34a4-4264-8012-d7b59958eda4",
   "metadata": {},
   "source": [
    "### Spatial & Temporal Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0882b0-18ae-4385-83ff-d5099f2da1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22 ms, sys: 2.94 ms, total: 24.9 ms\n",
      "Wall time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def bin_coordinates(df, precision=2):\n",
    "    \"\"\"Bins latitude and longitude to a specified precision.\"\"\"\n",
    "    df.loc[:, \"pickup_lat_bin\"] = np.round(df[\"pickup_latitude\"], precision)\n",
    "    df.loc[:, \"pickup_long_bin\"] = np.round(df[\"pickup_longitude\"], precision)\n",
    "    df.loc[:, \"dropoff_lat_bin\"] = np.round(df[\"dropoff_latitude\"], precision)\n",
    "    df.loc[:, \"dropoff_long_bin\"] = np.round(df[\"dropoff_longitude\"], precision)\n",
    "\n",
    "bin_coordinates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d8d85c-1778-45fd-b4a4-03ec690bcbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 184 ms, total: 17.2 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def compute_spatial_aggregations(df, min_trips=100):\n",
    "    \"\"\"Computes trip counts for different spatial aggregations.\"\"\"\n",
    "    groupings = [\n",
    "        [\"pickup_lat_bin\", \"pickup_long_bin\", \"dropoff_lat_bin\", \"dropoff_long_bin\"],\n",
    "        [\"pickup_lat_bin\", \"pickup_long_bin\"],\n",
    "        [\"dropoff_lat_bin\", \"dropoff_long_bin\"]\n",
    "    ]\n",
    "    \n",
    "    for groupby_cols in groupings:\n",
    "        col_name = \"cnt_coords_bin_\" + \"\".join(set([col[0] for col in groupby_cols]))\n",
    "        \n",
    "        # Compute trip counts and store in a dictionary for fast lookup\n",
    "        counts = df.groupby(groupby_cols).size().to_dict()\n",
    "        \n",
    "        # Apply counts to create a new column in the dataframe\n",
    "        df[col_name] = df[groupby_cols].apply(lambda row: counts.get(tuple(row), 0), axis=1)\n",
    "        \n",
    "        # Apply filtering based on min_trips\n",
    "        df[col_name] = df[col_name].where(df[col_name] >= min_trips, 0)\n",
    "\n",
    "compute_spatial_aggregations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf0fb4b8-b9ae-4dde-aa83-ccf3b18bdf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 16.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_chunk(chunk, df_ref):\n",
    "    \"\"\"\n",
    "    Process a chunk of the DataFrame to compute spatial-temporal features.\n",
    "    \n",
    "    Args:\n",
    "        chunk: A subset of the main DataFrame.\n",
    "        df_ref: Reference DataFrame for aggregation calculations.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed features for the chunk.\n",
    "    \"\"\"\n",
    "    # Ensure pickup_datetime is in datetime format\n",
    "    chunk[\"pickup_datetime\"] = pd.to_datetime(chunk[\"pickup_datetime\"])\n",
    "    df_ref[\"pickup_datetime\"] = pd.to_datetime(df_ref[\"pickup_datetime\"])\n",
    "    \n",
    "    # Create temporary columns for time calculations\n",
    "    df_ref = df_ref.copy()\n",
    "    df_ref[\"pickup_hour\"] = df_ref[\"pickup_datetime\"].dt.floor(\"H\")\n",
    "    \n",
    "    # Add new columns with default values\n",
    "    chunk[\"cnt_prev_1h\"] = 0\n",
    "    chunk[\"cnt_mean_prev_3h_pickups\"] = 0.0\n",
    "    chunk[\"cnt_mean_prev_3h_dropoffs\"] = 0.0\n",
    "    \n",
    "    for idx, row in chunk.iterrows():\n",
    "        # Get current trip attributes\n",
    "        current_time = row[\"pickup_datetime\"]\n",
    "        current_hour = current_time.floor(\"H\")\n",
    "        pl_bin = row[\"pickup_lat_bin\"]\n",
    "        plon_bin = row[\"pickup_long_bin\"]\n",
    "        dl_bin = row[\"dropoff_lat_bin\"]\n",
    "        dlon_bin = row[\"dropoff_long_bin\"]\n",
    "        \n",
    "        # Calculate 1-hour window\n",
    "        t1_start = current_hour - pd.Timedelta(hours=1)\n",
    "        t1_end = current_hour\n",
    "        \n",
    "        # Calculate 3-hour average window (T-4h to T-1h)\n",
    "        t3_start = current_hour - pd.Timedelta(hours=4)\n",
    "        t3_end = current_hour - pd.Timedelta(hours=1)\n",
    "        \n",
    "        # Get reference data subsets\n",
    "        ref_1h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t1_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t1_end)\n",
    "        ]\n",
    "        \n",
    "        ref_3h = df_ref[\n",
    "            (df_ref[\"pickup_hour\"] >= t3_start) & \n",
    "            (df_ref[\"pickup_hour\"] < t3_end)\n",
    "        ]\n",
    "        \n",
    "        # Calculate 1-hour total count\n",
    "        chunk.at[idx, \"cnt_prev_1h\"] = len(ref_1h)\n",
    "        \n",
    "        # Calculate 3-hour spatial averages\n",
    "        pickup_count = len(ref_3h[\n",
    "            (ref_3h[\"pickup_lat_bin\"] == pl_bin) &\n",
    "            (ref_3h[\"pickup_long_bin\"] == plon_bin)\n",
    "        ])\n",
    "        \n",
    "        dropoff_count = len(ref_3h[\n",
    "            (ref_3h[\"dropoff_lat_bin\"] == dl_bin) &\n",
    "            (ref_3h[\"dropoff_long_bin\"] == dlon_bin)\n",
    "        ])\n",
    "        \n",
    "        chunk.at[idx, \"cnt_mean_prev_3h_pickups\"] = pickup_count / 3\n",
    "        chunk.at[idx, \"cnt_mean_prev_3h_dropoffs\"] = dropoff_count / 3\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "def compute_spatial_temporal_aggregation_parallel(df, df_ref, n_jobs):\n",
    "    \"\"\"\n",
    "    Compute spatial-temporal aggregated features for taxi trips in parallel.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to compute features for (must contain pickup/dropoff bins and timestamps).\n",
    "        df_ref: Reference DataFrame used for aggregation calculations.\n",
    "        n_jobs: Number of parallel jobs to run. Default is -1 (use all available cores).\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with computed features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine chunk size based on available cores\n",
    "    num_chunks = min(n_jobs, len(df))\n",
    "    chunk_size = len(df) // num_chunks if num_chunks > 0 else len(df)\n",
    "    print(f\"Available CPU Core: {n_jobs} | Chunk Size: {chunk_size}\")\n",
    "    \n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_chunk)(chunk, df_ref) for chunk in chunks\n",
    "    )\n",
    "    \n",
    "    # Combine results into a single DataFrame\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a572f3-82ee-4677-906a-9d34a9ce85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU Core: 8 | Chunk Size: 151373\n",
      "CPU times: user 42.9 s, sys: 22.7 s, total: 1min 5s\n",
      "Wall time: 55min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Apply Spatial Temporal Aggregation Paralleled Calculation:\n",
    "df = compute_spatial_temporal_aggregation_parallel(df, df, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1f08b-7543-49f5-89bd-1bb66593419e",
   "metadata": {},
   "source": [
    "### OSRM Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68f54309-c493-4745-bad2-2277616c3ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1458643 entries, 0 to 1458642\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count    Dtype  \n",
      "---  ------             --------------    -----  \n",
      " 0   id                 1458643 non-null  object \n",
      " 1   total_distance     1458643 non-null  float64\n",
      " 2   total_travel_time  1458643 non-null  float64\n",
      " 3   number_of_steps    1458643 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 44.5+ MB\n",
      "CPU times: user 2.87 s, sys: 1.42 s, total: 4.29 s\n",
      "Wall time: 3.99 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Credit: The OSRM-based routing data used here was generated by Oscarleo \n",
    "# and is available at: https://www.kaggle.com/datasets/oscarleo/new-york-city-taxi-with-osrm\n",
    "\n",
    "# Load the Parquet file with selected columns\n",
    "fastest_routes = pd.read_parquet(\n",
    "    \"data/osrm/fastest_routes.parquet\",\n",
    "    columns=['id', 'total_distance', 'total_travel_time', 'number_of_steps']\n",
    ")\n",
    "\n",
    "# Merge on 'id' the key column\n",
    "df = df.merge(fastest_routes, on=\"id\", how=\"left\")\n",
    "\n",
    "# Brief overview of fastest routes planed by osrm framework \n",
    "fastest_routes.info()\n",
    "\n",
    "# Delte relevant variables\n",
    "del fastest_routes\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b56ba-7844-46ba-9ac0-fbddc3421355",
   "metadata": {},
   "source": [
    "### Drop Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75d0b4d-aae2-4d44-80f8-96cb84a7ec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'vendor_id', 'pickup_datetime', 'passenger_count',\n",
       "       'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
       "       'dropoff_latitude', 'store_and_fwd_flag', 'trip_duration',\n",
       "       'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1',\n",
       "       'geodesic_distance', 'pickup_hour_of_day', 'day_of_week',\n",
       "       'hour_of_week', 'month_of_year', 'day_of_year', 'week_of_year',\n",
       "       'hour_of_year', 'pickup_date', 'date', 'r_depth', 's_fall', 's_depth',\n",
       "       'all_precip', 'has_snow', 'has_rain', 'max_temp', 'min_temp', 'speed',\n",
       "       'pickup_lat_bin', 'pickup_long_bin', 'dropoff_lat_bin',\n",
       "       'dropoff_long_bin', 'cnt_coords_bin_pd', 'cnt_coords_bin_p',\n",
       "       'cnt_coords_bin_d', 'cnt_prev_1h', 'cnt_mean_prev_3h_pickups',\n",
       "       'cnt_mean_prev_3h_dropoffs', 'total_distance', 'total_travel_time',\n",
       "       'number_of_steps'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "654d2eb3-6aed-4188-8b43-e192c00aa5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 350 ms, sys: 0 ns, total: 350 ms\n",
      "Wall time: 349 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'vendor_id', 'passenger_count', 'store_and_fwd_flag',\n",
       "       'pickup_pca0', 'pickup_pca1', 'dropoff_pca0', 'dropoff_pca1',\n",
       "       'geodesic_distance', 'pickup_hour_of_day', 'day_of_week',\n",
       "       'hour_of_week', 'month_of_year', 'day_of_year', 'week_of_year',\n",
       "       'hour_of_year', 'r_depth', 's_fall', 's_depth', 'all_precip',\n",
       "       'has_snow', 'has_rain', 'max_temp', 'min_temp', 'cnt_coords_bin_pd',\n",
       "       'cnt_coords_bin_p', 'cnt_coords_bin_d', 'cnt_prev_1h',\n",
       "       'cnt_mean_prev_3h_pickups', 'cnt_mean_prev_3h_dropoffs',\n",
       "       'total_distance', 'total_travel_time', 'number_of_steps',\n",
       "       'trip_duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Delete redundant, intermediate columns\n",
    "df.drop(columns=[\n",
    "    \"pickup_datetime\", \"pickup_date\", \"date\", \"speed\",\n",
    "    \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "    \"pickup_lat_bin\", \"pickup_long_bin\", \"dropoff_lat_bin\", \"dropoff_long_bin\"\n",
    "], inplace=True)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# Reorganize the columns to make `trip_duration` the target column in the end\n",
    "df = df[[col for col in df.columns if col != \"trip_duration\"] + [\"trip_duration\"]]\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b508b61-1573-4d18-b45d-88e54695ba2e",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0a8e396-b9a8-4879-bfe9-761239f1da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 585 ms, sys: 0 ns, total: 585 ms\n",
      "Wall time: 582 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "bool_columns = [\"has_snow\", \"has_rain\"]\n",
    "df[bool_columns] = df[bool_columns].astype(int)\n",
    "\n",
    "# Process the vendor_id column\n",
    "if \"vendor_id\" in df.columns:\n",
    "    df[\"vendor_id\"] = df[\"vendor_id\"] - 1\n",
    "    \n",
    "# Progress the flag column\n",
    "df[\"store_and_fwd_flag\"] = df[\"store_and_fwd_flag\"].apply(lambda x: 0 if x == \"Y\" else 1)\n",
    "df[\"store_and_fwd_flag\"] = df[\"store_and_fwd_flag\"].apply(lambda x: 0 if x == \"Y\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3972ea58-f46a-428e-ae1e-61c3c126cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1210987 entries, 0 to 1210986\n",
      "Data columns (total 34 columns):\n",
      " #   Column                     Non-Null Count    Dtype  \n",
      "---  ------                     --------------    -----  \n",
      " 0   id                         1210987 non-null  object \n",
      " 1   vendor_id                  1210987 non-null  int64  \n",
      " 2   passenger_count            1210987 non-null  int64  \n",
      " 3   store_and_fwd_flag         1210987 non-null  int64  \n",
      " 4   pickup_pca0                1210987 non-null  float64\n",
      " 5   pickup_pca1                1210987 non-null  float64\n",
      " 6   dropoff_pca0               1210987 non-null  float64\n",
      " 7   dropoff_pca1               1210987 non-null  float64\n",
      " 8   geodesic_distance          1210987 non-null  float64\n",
      " 9   pickup_hour_of_day         1210987 non-null  int32  \n",
      " 10  day_of_week                1210987 non-null  int64  \n",
      " 11  hour_of_week               1210987 non-null  int64  \n",
      " 12  month_of_year              1210987 non-null  int64  \n",
      " 13  day_of_year                1210987 non-null  int64  \n",
      " 14  week_of_year               1210987 non-null  int64  \n",
      " 15  hour_of_year               1210987 non-null  int64  \n",
      " 16  r_depth                    1210987 non-null  float64\n",
      " 17  s_fall                     1210987 non-null  float64\n",
      " 18  s_depth                    1210987 non-null  float64\n",
      " 19  all_precip                 1210987 non-null  float64\n",
      " 20  has_snow                   1210987 non-null  int64  \n",
      " 21  has_rain                   1210987 non-null  int64  \n",
      " 22  max_temp                   1210987 non-null  int64  \n",
      " 23  min_temp                   1210987 non-null  int64  \n",
      " 24  cnt_coords_bin_pd          1210987 non-null  int64  \n",
      " 25  cnt_coords_bin_p           1210987 non-null  int64  \n",
      " 26  cnt_coords_bin_d           1210987 non-null  int64  \n",
      " 27  cnt_prev_1h                1210987 non-null  int64  \n",
      " 28  cnt_mean_prev_3h_pickups   1210987 non-null  float64\n",
      " 29  cnt_mean_prev_3h_dropoffs  1210987 non-null  float64\n",
      " 30  total_distance             1210987 non-null  float64\n",
      " 31  total_travel_time          1210987 non-null  float64\n",
      " 32  number_of_steps            1210987 non-null  int64  \n",
      " 33  trip_duration              1210987 non-null  int64  \n",
      "dtypes: float64(13), int32(1), int64(19), object(1)\n",
      "memory usage: 309.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb58e6a-b540-44f1-9783-1cab1fed3c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>pickup_pca0</th>\n",
       "      <th>pickup_pca1</th>\n",
       "      <th>dropoff_pca0</th>\n",
       "      <th>dropoff_pca1</th>\n",
       "      <th>geodesic_distance</th>\n",
       "      <th>pickup_hour_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>cnt_coords_bin_pd</th>\n",
       "      <th>cnt_coords_bin_p</th>\n",
       "      <th>cnt_coords_bin_d</th>\n",
       "      <th>cnt_prev_1h</th>\n",
       "      <th>cnt_mean_prev_3h_pickups</th>\n",
       "      <th>cnt_mean_prev_3h_dropoffs</th>\n",
       "      <th>total_distance</th>\n",
       "      <th>total_travel_time</th>\n",
       "      <th>number_of_steps</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1210987.0</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "      <td>1.210987e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.345714e-01</td>\n",
       "      <td>1.669153e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.675423e-03</td>\n",
       "      <td>-4.419350e-02</td>\n",
       "      <td>-7.728207e-03</td>\n",
       "      <td>-1.712308e-02</td>\n",
       "      <td>3.952992e+00</td>\n",
       "      <td>1.370683e+01</td>\n",
       "      <td>3.040424e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166138e+03</td>\n",
       "      <td>3.669627e+04</td>\n",
       "      <td>2.893738e+04</td>\n",
       "      <td>3.327892e+02</td>\n",
       "      <td>9.922165e+00</td>\n",
       "      <td>7.881916e+00</td>\n",
       "      <td>5.319821e+03</td>\n",
       "      <td>4.415067e+02</td>\n",
       "      <td>8.219698e+00</td>\n",
       "      <td>9.465372e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.988036e-01</td>\n",
       "      <td>1.314138e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.628665e-01</td>\n",
       "      <td>8.420306e-01</td>\n",
       "      <td>5.354075e-01</td>\n",
       "      <td>9.688054e-01</td>\n",
       "      <td>4.131656e+00</td>\n",
       "      <td>6.369440e+00</td>\n",
       "      <td>1.943954e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.293731e+03</td>\n",
       "      <td>2.284563e+04</td>\n",
       "      <td>2.114191e+04</td>\n",
       "      <td>1.008178e+02</td>\n",
       "      <td>8.313414e+00</td>\n",
       "      <td>7.627852e+00</td>\n",
       "      <td>5.541506e+03</td>\n",
       "      <td>3.184298e+02</td>\n",
       "      <td>4.509629e+00</td>\n",
       "      <td>6.489072e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.046855e+00</td>\n",
       "      <td>-6.539942e+00</td>\n",
       "      <td>-1.817656e+01</td>\n",
       "      <td>-6.590078e+00</td>\n",
       "      <td>3.065172e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.010000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-8.788089e-02</td>\n",
       "      <td>-4.178586e-01</td>\n",
       "      <td>-1.567154e-01</td>\n",
       "      <td>-4.799290e-01</td>\n",
       "      <td>1.581162e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.880000e+02</td>\n",
       "      <td>1.957500e+04</td>\n",
       "      <td>1.146700e+04</td>\n",
       "      <td>2.950000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.666667e+00</td>\n",
       "      <td>2.118500e+03</td>\n",
       "      <td>2.267000e+02</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.080000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.146166e-01</td>\n",
       "      <td>5.761965e-02</td>\n",
       "      <td>8.902504e-02</td>\n",
       "      <td>6.947418e-02</td>\n",
       "      <td>2.499565e+00</td>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>8.230000e+02</td>\n",
       "      <td>3.377200e+04</td>\n",
       "      <td>2.662400e+04</td>\n",
       "      <td>3.440000e+02</td>\n",
       "      <td>7.666667e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>3.269200e+03</td>\n",
       "      <td>3.379000e+02</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>7.580000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.712265e-01</td>\n",
       "      <td>4.323915e-01</td>\n",
       "      <td>2.596224e-01</td>\n",
       "      <td>4.865418e-01</td>\n",
       "      <td>4.481081e+00</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.652000e+03</td>\n",
       "      <td>5.575200e+04</td>\n",
       "      <td>4.332500e+04</td>\n",
       "      <td>3.970000e+02</td>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>6.007200e+03</td>\n",
       "      <td>5.497000e+02</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.165000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.045628e+00</td>\n",
       "      <td>1.675372e+01</td>\n",
       "      <td>1.055469e+01</td>\n",
       "      <td>2.075788e+01</td>\n",
       "      <td>1.166143e+02</td>\n",
       "      <td>2.300000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.912000e+03</td>\n",
       "      <td>7.604700e+04</td>\n",
       "      <td>6.533900e+04</td>\n",
       "      <td>5.440000e+02</td>\n",
       "      <td>5.033333e+01</td>\n",
       "      <td>4.900000e+01</td>\n",
       "      <td>8.506430e+04</td>\n",
       "      <td>4.243600e+03</td>\n",
       "      <td>4.600000e+01</td>\n",
       "      <td>2.040000e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          vendor_id  passenger_count  store_and_fwd_flag   pickup_pca0  \\\n",
       "count  1.210987e+06     1.210987e+06           1210987.0  1.210987e+06   \n",
       "mean   5.345714e-01     1.669153e+00                 1.0 -6.675423e-03   \n",
       "std    4.988036e-01     1.314138e+00                 0.0  5.628665e-01   \n",
       "min    0.000000e+00     0.000000e+00                 1.0 -6.046855e+00   \n",
       "25%    0.000000e+00     1.000000e+00                 1.0 -8.788089e-02   \n",
       "50%    1.000000e+00     1.000000e+00                 1.0  1.146166e-01   \n",
       "75%    1.000000e+00     2.000000e+00                 1.0  2.712265e-01   \n",
       "max    1.000000e+00     6.000000e+00                 1.0  5.045628e+00   \n",
       "\n",
       "        pickup_pca1  dropoff_pca0  dropoff_pca1  geodesic_distance  \\\n",
       "count  1.210987e+06  1.210987e+06  1.210987e+06       1.210987e+06   \n",
       "mean  -4.419350e-02 -7.728207e-03 -1.712308e-02       3.952992e+00   \n",
       "std    8.420306e-01  5.354075e-01  9.688054e-01       4.131656e+00   \n",
       "min   -6.539942e+00 -1.817656e+01 -6.590078e+00       3.065172e-01   \n",
       "25%   -4.178586e-01 -1.567154e-01 -4.799290e-01       1.581162e+00   \n",
       "50%    5.761965e-02  8.902504e-02  6.947418e-02       2.499565e+00   \n",
       "75%    4.323915e-01  2.596224e-01  4.865418e-01       4.481081e+00   \n",
       "max    1.675372e+01  1.055469e+01  2.075788e+01       1.166143e+02   \n",
       "\n",
       "       pickup_hour_of_day   day_of_week  ...  cnt_coords_bin_pd  \\\n",
       "count        1.210987e+06  1.210987e+06  ...       1.210987e+06   \n",
       "mean         1.370683e+01  3.040424e+00  ...       1.166138e+03   \n",
       "std          6.369440e+00  1.943954e+00  ...       1.293731e+03   \n",
       "min          0.000000e+00  0.000000e+00  ...       0.000000e+00   \n",
       "25%          9.000000e+00  1.000000e+00  ...       1.880000e+02   \n",
       "50%          1.400000e+01  3.000000e+00  ...       8.230000e+02   \n",
       "75%          1.900000e+01  5.000000e+00  ...       1.652000e+03   \n",
       "max          2.300000e+01  6.000000e+00  ...       6.912000e+03   \n",
       "\n",
       "       cnt_coords_bin_p  cnt_coords_bin_d   cnt_prev_1h  \\\n",
       "count      1.210987e+06      1.210987e+06  1.210987e+06   \n",
       "mean       3.669627e+04      2.893738e+04  3.327892e+02   \n",
       "std        2.284563e+04      2.114191e+04  1.008178e+02   \n",
       "min        0.000000e+00      0.000000e+00  0.000000e+00   \n",
       "25%        1.957500e+04      1.146700e+04  2.950000e+02   \n",
       "50%        3.377200e+04      2.662400e+04  3.440000e+02   \n",
       "75%        5.575200e+04      4.332500e+04  3.970000e+02   \n",
       "max        7.604700e+04      6.533900e+04  5.440000e+02   \n",
       "\n",
       "       cnt_mean_prev_3h_pickups  cnt_mean_prev_3h_dropoffs  total_distance  \\\n",
       "count              1.210987e+06               1.210987e+06    1.210987e+06   \n",
       "mean               9.922165e+00               7.881916e+00    5.319821e+03   \n",
       "std                8.313414e+00               7.627852e+00    5.541506e+03   \n",
       "min                0.000000e+00               0.000000e+00    0.000000e+00   \n",
       "25%                3.000000e+00               1.666667e+00    2.118500e+03   \n",
       "50%                7.666667e+00               6.000000e+00    3.269200e+03   \n",
       "75%                1.500000e+01               1.200000e+01    6.007200e+03   \n",
       "max                5.033333e+01               4.900000e+01    8.506430e+04   \n",
       "\n",
       "       total_travel_time  number_of_steps  trip_duration  \n",
       "count       1.210987e+06     1.210987e+06   1.210987e+06  \n",
       "mean        4.415067e+02     8.219698e+00   9.465372e+02  \n",
       "std         3.184298e+02     4.509629e+00   6.489072e+02  \n",
       "min         0.000000e+00     2.000000e+00   3.010000e+02  \n",
       "25%         2.267000e+02     5.000000e+00   5.080000e+02  \n",
       "50%         3.379000e+02     7.000000e+00   7.580000e+02  \n",
       "75%         5.497000e+02     1.000000e+01   1.165000e+03  \n",
       "max         4.243600e+03     4.600000e+01   2.040000e+04  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f507a63-4abe-4eb0-99d7-cb478c3dffa1",
   "metadata": {},
   "source": [
    "### Save Data in parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "937ff5b0-54a4-4492-8b0b-de0b3751b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in train set: 968789\n",
      "Number of records in validation set: 121099\n",
      "Number of records in test set: 121099\n",
      "CPU times: user 3.4 s, sys: 448 ms, total: 3.85 s\n",
      "Wall time: 3.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Ensure the 'prep' directory exists\n",
    "os.makedirs(os.path.join(\"data\", \"prep\"), exist_ok=True)\n",
    "\n",
    "# Save to preprocessed df as Parquet files\n",
    "df.to_parquet(\"data/prep/data.parquet\", index=False)\n",
    "\n",
    "# Perform 80-20 train-validation-test split (80% train, 10% validation, 10% test)\n",
    "df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save the train, validation, and test datasets as Parquet files\n",
    "df_train.to_parquet(\"data/prep/data_train.parquet\", index=False)\n",
    "df_valid.to_parquet(\"data/prep/data_valid.parquet\", index=False)\n",
    "df_test.to_parquet(\"data/prep/data_test.parquet\", index=False)\n",
    "\n",
    "# Print the number of records in each set\n",
    "print(f\"Number of records in train set: {len(df_train)}\")\n",
    "print(f\"Number of records in validation set: {len(df_valid)}\")\n",
    "print(f\"Number of records in test set: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea8ccf-0549-4236-a9fc-937cf4a11fe1",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
